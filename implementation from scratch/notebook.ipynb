{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h1 style=\"text-align:center;\">Transformer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### INITIAL SETUP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:39:24.492167: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-03 14:39:24.493841: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-03 14:39:24.527477: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-03 14:39:24.528458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 14:39:25.063880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "# Hyperpararmeters for the model\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Hyperparameters for the training\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### DATASET LOADER\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class for the dataset\n",
    "class PrepareDataset:\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialization\n",
    "        self.train_split = 0.8        # Train split ratio\n",
    "        self.val_split = 0.1          # Validation split ratio\n",
    "\n",
    "    # Function for creating and fitting a tokenizer to given dataset\n",
    "    def create_tokenizer(self, dataset):\n",
    "        \"\"\"\n",
    "        This function creates and fits a tokenizer to given dataset.\n",
    "\n",
    "        PARAMETERS\n",
    "        ===========================\n",
    "            - dataset (list): list of sentences\n",
    "\n",
    "        RETURNS\n",
    "        ===========================\n",
    "            - tokenizer (tensorflow tokenizer): fitted tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize a tokenizer\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "        # Fit the tokenizer to the dataset\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "    \n",
    "    # Function for finding the sequence length of the dataset\n",
    "    def find_sequence_length(self, dataset):\n",
    "        \"\"\"\n",
    "        This function finds the sequence length of the dataset.\n",
    "\n",
    "        PARAMETERS\n",
    "        ===========================\n",
    "            - dataset (list): list of sentences\n",
    "\n",
    "        RETURNS\n",
    "        ===========================\n",
    "            - sequence_length (int): sequence length of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # Sequence length\n",
    "        sequence_length = max([len(i_seq.split()) for i_seq in dataset])\n",
    "\n",
    "        return sequence_length\n",
    "    \n",
    "    # Function for finding the vocabulary size\n",
    "    def find_vocabulary_size(self, tokenizer, dataset):\n",
    "        \"\"\"\n",
    "        This function finds the vocabulary size of the dataset.\n",
    "\n",
    "        PARAMETERS\n",
    "        ===========================\n",
    "            - tokenizer (tensorflow tokenizer): fitted tokenizer\n",
    "            - dataset (list): list of sentences\n",
    "\n",
    "        RETURNS\n",
    "        ===========================\n",
    "            - vocabulary_size (int): vocabulary size of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # Fit the tokenizer to the dataset\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        # Vocabulary size\n",
    "        vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        return vocabulary_size\n",
    "    \n",
    "    # Function for encoding and padding the input sequences\n",
    "    def encode_and_pad(self, dataset, tokenizer, sequence_length):\n",
    "        \"\"\"\n",
    "        This function encodes and pads the input sequences.\n",
    "\n",
    "        PARAMETERS\n",
    "        ===========================\n",
    "            - dataset (list): list of sentences\n",
    "            - tokenizer (tensorflow tokenizer): fitted tokenizer\n",
    "            - sequence_length (int): sequence length of the dataset\n",
    "\n",
    "        RETURNS\n",
    "        ===========================\n",
    "            - out (tensorflow tensor): encoded and padded dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # Encode the dataset\n",
    "        encoded_dataset = tokenizer.texts_to_sequences(dataset)\n",
    "\n",
    "        # Pad the dataset\n",
    "        padded_dataset = tf.keras.preprocessing.sequence.pad_sequences(encoded_dataset, maxlen = sequence_length, padding = \"post\")\n",
    "\n",
    "        # Convert the dataset into tensor\n",
    "        out = tf.convert_to_tensor(padded_dataset, dtype=tf.int64)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # Function for saving the tokenizer\n",
    "    def save_tokenizer(self, tokenizer, name):\n",
    "        \"\"\"\n",
    "        This function saves the tokenizer into a pickle file.\n",
    "\n",
    "        PARAMETERS\n",
    "        ===========================\n",
    "            - tokenizer (tensorflow tokenizer): fitted tokenizer\n",
    "            - name (str): name of the pickle file   \n",
    "\n",
    "        RETURNS \n",
    "        ===========================\n",
    "            - None\n",
    "        \"\"\"\n",
    "        # Open the pickle file\n",
    "        with open(name + \"_tokenizer.pkl\", \"wb\") as f:\n",
    "\n",
    "            # Dump the tokenizer\n",
    "            pickle.dump(tokenizer, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, filename, **kwargs):\n",
    "                 \n",
    "        # Load the dataset (already cleaned)\n",
    "        dataset_clean = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "        # TODO: Sample a subset of dataset\n",
    "        # self.n_sentences = 10_000                  \n",
    "        self.n_sentences = len(dataset_clean)\n",
    "\n",
    "        # Sample a subset of the dataset\n",
    "        dataset = dataset_clean[:self.n_sentences, :]\n",
    "\n",
    "        # Add the START and EOS tokens to each sentence\n",
    "        for i in range(dataset[:, 0].size):\n",
    "\n",
    "            # Add the tokens to the sentences \n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        # Split the dataset into train, val, test\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        val   = dataset[int(self.n_sentences * self.train_split)  :  int(self.n_sentences * (1 - self.val_split))]\n",
    "        test  = dataset[int(self.n_sentences * (1 - self.val_split)) :]\n",
    "\n",
    "        # Tokenization process for encoder input\n",
    "        encoder_tokenizer       = self.create_tokenizer(dataset[:, 0])\n",
    "        encoder_sequence_length = self.find_sequence_length(dataset[:, 0])\n",
    "        encoder_vocabulary_size = self.find_vocabulary_size(encoder_tokenizer, train[:, 0])\n",
    "\n",
    "        # Tokenization process for decoder input\n",
    "        decoder_tokenizer       = self.create_tokenizer(dataset[:, 1])\n",
    "        decoder_sequence_length = self.find_sequence_length(dataset[:, 1])\n",
    "        decoder_vocabulary_size = self.find_vocabulary_size(decoder_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the train dataset\n",
    "        train_x = self.encode_and_pad(train[:, 0], encoder_tokenizer, encoder_sequence_length)\n",
    "        train_y = self.encode_and_pad(train[:, 1], decoder_tokenizer, decoder_sequence_length)\n",
    "\n",
    "        # Encode and pad the validation dataset\n",
    "        val_x = self.encode_and_pad(val[:, 0], encoder_tokenizer, encoder_sequence_length)\n",
    "        val_y = self.encode_and_pad(val[:, 1], decoder_tokenizer, decoder_sequence_length)\n",
    "\n",
    "        # Save the encoder/decoder tokenizer\n",
    "        self.save_tokenizer(encoder_tokenizer, \"./saved/encoder\")\n",
    "        self.save_tokenizer(decoder_tokenizer, \"./saved/decoder\")\n",
    "\n",
    "        # Save the testing dataset into a text file using savetxt\n",
    "        np.savetxt(\"./saved/test_dataset.txt\", test, fmt=\"%s\")\n",
    "\n",
    "        return (train_x, train_y, val_x, val_y, train, val, encoder_sequence_length, decoder_sequence_length, encoder_vocabulary_size, decoder_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape:  (8000, 7)\n",
      "Train Y shape:  (8000, 12)\n",
      "Val X shape:  (1000, 7)\n",
      "Val Y shape:  (1000, 12)\n",
      "Train shape:  (8000, 2)\n",
      "Val shape:  (1000, 2)\n",
      "Encoder sequence length:  7\n",
      "Decoder sequence length:  12\n",
      "Encoder vocabulary size:  2404\n",
      "Decoder vocabulary size:  3864\n",
      "Train X:  tf.Tensor(\n",
      "[[   1    4    7 ... 1095    2    0]\n",
      " [   1   21    7 ...    5    2    0]\n",
      " [   1 1508    4 ...    0    0    0]\n",
      " ...\n",
      " [   1    4    7 ...    2    0    0]\n",
      " [   1  221    6 ...    2    0    0]\n",
      " [   1   22   19 ...  256    2    0]], shape=(8000, 7), dtype=int64)\n",
      "Train Y:  tf.Tensor(\n",
      "[[   1    5    4 ...    0    0    0]\n",
      " [   1    8    4 ...    0    0    0]\n",
      " [   1 1986   21 ...    0    0    0]\n",
      " ...\n",
      " [   1    5    4 ...    0    0    0]\n",
      " [   1 3464    7 ...    0    0    0]\n",
      " [   1    8   26 ...    0    0    0]], shape=(8000, 12), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:39:26.617683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-03 14:39:26.619693: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Test out the codes\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the dataset class\n",
    "    dataset = PrepareDataset()\n",
    "\n",
    "    # Call it \n",
    "    train_x, train_y, val_x, val_y, train, val, encoder_sequence_length, decoder_sequence_length, encoder_vocabulary_size, decoder_vocabulary_size = dataset(\"./dataset/english-german-both.pkl\")\n",
    "\n",
    "    # Report\n",
    "    print(\"Train X shape: \", train_x.shape)\n",
    "    print(\"Train Y shape: \", train_y.shape)\n",
    "    print(\"Val X shape: \", val_x.shape)\n",
    "    print(\"Val Y shape: \", val_y.shape)\n",
    "    print(\"Train shape: \", train.shape)\n",
    "    print(\"Val shape: \", val.shape)\n",
    "    print(\"Encoder sequence length: \", encoder_sequence_length)\n",
    "    print(\"Decoder sequence length: \", decoder_sequence_length)\n",
    "    print(\"Encoder vocabulary size: \", encoder_vocabulary_size)\n",
    "    print(\"Decoder vocabulary size: \", decoder_vocabulary_size)\n",
    "    print(\"Train X: \", train_x)\n",
    "    print(\"Train Y: \", train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### POSITIONAL ENCODING\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POSITION EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POsition embedding layer\n",
    "class PositionEmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.word_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=output_dim)\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=output_dim)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Initialize the positions\n",
    "        position_indices = tf.range(start=0, limit=tf.shape(inputs)[-1])\n",
    "\n",
    "        # Feed words and positions to embedding layer\n",
    "        embedded_words     = self.word_embedding_layer(inputs)\n",
    "        embedded_positions = self.position_embedding_layer(position_indices)\n",
    "\n",
    "        # Sum up the embeddings\n",
    "        out = embedded_words + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from my_embedded_layer:  tf.Tensor(\n",
      "[[[-2.4924435e-02  5.5890642e-02  3.3287253e-02  1.5423108e-02\n",
      "    3.0362286e-02 -3.5833754e-02]\n",
      "  [ 5.1383585e-02 -4.6783574e-03  2.8037265e-02  1.4657751e-02\n",
      "    3.2689061e-02 -8.0875993e-02]\n",
      "  [ 4.6030592e-02  4.2018507e-02  1.4287449e-02  3.3984926e-02\n",
      "    2.1658257e-02 -8.8141680e-02]\n",
      "  [-3.2368265e-03 -2.8120220e-02 -1.2495413e-02 -1.1844039e-03\n",
      "    2.5623802e-02  2.8911199e-02]\n",
      "  [-5.9241205e-03 -1.9663963e-02  3.4830939e-02 -2.6283884e-02\n",
      "   -6.4223610e-02  1.4392287e-04]]\n",
      "\n",
      " [[ 1.5024196e-02  1.5852045e-02 -5.0491825e-02 -3.6675327e-02\n",
      "    8.6241961e-02  4.7145970e-03]\n",
      "  [-1.2702819e-02  3.6388449e-02  7.3169746e-02  9.1476686e-02\n",
      "    4.3241873e-02  7.8938454e-03]\n",
      "  [-2.6690863e-02  9.6485727e-03 -2.5635362e-03 -2.4817266e-02\n",
      "    2.1782406e-03 -5.0211728e-02]\n",
      "  [ 8.1039965e-05 -2.1956801e-02  4.1464616e-02 -1.3654804e-02\n",
      "   -8.9165177e-03  6.4674452e-02]\n",
      "  [-5.9241205e-03 -1.9663963e-02  3.4830939e-02 -2.6283884e-02\n",
      "   -6.4223610e-02  1.4392287e-04]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:39:26.834346: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    output_length = 6\n",
    "    output_sequence_length = 5\n",
    "    vocab_size = 10\n",
    "\n",
    "    # Sample text\n",
    "    sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "\n",
    "    # Convert to tf.data\n",
    "    sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "    # Convert all sentences to tensors\n",
    "    word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "    \n",
    "    # Vectorize the text\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length, max_tokens=vocab_size)\n",
    "    vectorize_layer.adapt(sentence_data)\n",
    "    vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "    # Feed to the embedding layer\n",
    "    my_embedding_layer = PositionEmbeddingLayer(output_sequence_length, vocab_size, output_length)\n",
    "    embedded_layer_output = my_embedding_layer(vectorized_words)\n",
    "    \n",
    "    # Report\n",
    "    print(\"Output from my_embedded_layer: \", embedded_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POSITION EMBEDDING WITH FIXED WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding layer class with initializing the weights with sine/cosine function\n",
    "class PositionEmbeddingLayerWithFixedWeights(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "\n",
    "        # Inherite parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the word / position embedding matrix (for initializing the weights with them)\n",
    "        word_embedding_matrix     = self.get_position_encoding(vocab_size, output_dim)\n",
    "        position_embedding_matrix = self.get_position_encoding(seq_length, output_dim)\n",
    "\n",
    "        # Initialize the word / position embedding layer\n",
    "        self.word_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                                              output_dim=output_dim,\n",
    "                                                              weights=[word_embedding_matrix],\n",
    "                                                              trainable=False)\n",
    "        self.position_embedding_layer = tf.keras.layers.Embedding(input_dim=seq_length,\n",
    "                                                                  output_dim=output_dim,\n",
    "                                                                  weights=[position_embedding_matrix],\n",
    "                                                                  trainable=False\n",
    "                                                                  )\n",
    "        \n",
    "    # Get position encoding (sine/cosine)\n",
    "    def get_position_encoding(self, seq_len, d, n=10_000):\n",
    "\n",
    "        # Initialize the positional matrix\n",
    "        P = np.zeros(shape=(seq_len, d))\n",
    "\n",
    "        # Loop over the range of sequence length\n",
    "        for k in range(seq_len):\n",
    "\n",
    "            # Loop over the index from 0 to d/2\n",
    "            for i in np.arange(0, int(d/2)):\n",
    "\n",
    "                # Denominator\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "\n",
    "                # Sine\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "\n",
    "                # Cosine\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "\n",
    "        return P\n",
    "    \n",
    "    # Call function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Initialize the position indices\n",
    "        position_indices = tf.range(start=0, limit=tf.shape(inputs)[-1])\n",
    "\n",
    "        # Feed the word and position data into the embedding layer\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_positions = self.position_embedding_layer(position_indices)\n",
    "\n",
    "        # Sum up the embedding layers\n",
    "        out = embedded_words + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:39:27.012522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from my_embedded_layer:  tf.Tensor(\n",
      "[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n",
      "    1.9999421 ]\n",
      "  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n",
      "    1.9999142 ]\n",
      "  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n",
      "    1.999877  ]\n",
      "  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n",
      "    1.9999698 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]\n",
      "\n",
      " [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n",
      "    1.9999605 ]\n",
      "  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n",
      "    1.9999814 ]\n",
      "  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    output_length = 6\n",
    "    output_sequence_length = 5\n",
    "    vocab_size = 10\n",
    "\n",
    "    # Sample text\n",
    "    sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "\n",
    "    # Convert to tf.data\n",
    "    sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "    \n",
    "    # Convert all sentences to tensors\n",
    "    word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "    \n",
    "    # Vectorize the text\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length, max_tokens=vocab_size)\n",
    "    vectorize_layer.adapt(sentence_data)\n",
    "    vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "    # Feed to the embedding layer\n",
    "    attnisallyouneed_embedding = PositionEmbeddingLayerWithFixedWeights(output_sequence_length, vocab_size, output_length)\n",
    "    attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n",
    "    \n",
    "    # Report\n",
    "    print(\"Output from my_embedded_layer: \", attnisallyouneed_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ATTENTION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention class\n",
    "class ScaledDotProductAttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    # Call function\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "\n",
    "        # Attention score\n",
    "        attention_score = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_score += (-1e9 * mask)\n",
    "\n",
    "        # Apply softmax\n",
    "        weights = tf.keras.backend.softmax(attention_score)\n",
    "\n",
    "        # Calculate the weighted sum \n",
    "        out = tf.matmul(weights, values)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the attention layer:  tf.Tensor(\n",
      "[[[0.4975923  0.40308696 0.5376995  ... 0.43953294 0.40770963 0.5057981 ]\n",
      "  [0.49572456 0.41930145 0.54521567 ... 0.46037316 0.40874925 0.49132714]\n",
      "  [0.48554596 0.4018998  0.54402006 ... 0.44024646 0.4166857  0.5141643 ]\n",
      "  [0.50080734 0.41860065 0.5461656  ... 0.44185263 0.41294837 0.49832794]\n",
      "  [0.5152608  0.40915662 0.52903247 ... 0.43702582 0.39840442 0.50156784]]\n",
      "\n",
      " [[0.44245288 0.18936467 0.59702444 ... 0.49811876 0.5584522  0.5803972 ]\n",
      "  [0.4383108  0.18972366 0.61596805 ... 0.48022068 0.55336654 0.5747172 ]\n",
      "  [0.44158235 0.18540372 0.61093885 ... 0.4716351  0.5642079  0.56254625]\n",
      "  [0.4432204  0.1895485  0.6068978  ... 0.47909385 0.5616576  0.56953514]\n",
      "  [0.43783954 0.19278497 0.6065576  ... 0.48845467 0.5533936  0.5799821 ]]\n",
      "\n",
      " [[0.5352766  0.5337863  0.59896123 ... 0.48238063 0.5877674  0.62280047]\n",
      "  [0.5182316  0.55643034 0.61300975 ... 0.47924885 0.56121916 0.611951  ]\n",
      "  [0.5643305  0.5348186  0.5899037  ... 0.4535912  0.6183972  0.6307318 ]\n",
      "  [0.52251464 0.53642756 0.60596937 ... 0.4885842  0.57043934 0.6137445 ]\n",
      "  [0.52450746 0.52390397 0.6042037  ... 0.4830554  0.593251   0.6104768 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.68191934 0.43571618 0.50895065 ... 0.6384728  0.44571668 0.49502286]\n",
      "  [0.67644936 0.43968332 0.5250914  ... 0.65382284 0.43602178 0.504457  ]\n",
      "  [0.68747383 0.4283403  0.5061722  ... 0.651726   0.45463547 0.49017784]\n",
      "  [0.68350667 0.41508636 0.4940398  ... 0.6190599  0.4447594  0.51065505]\n",
      "  [0.67636853 0.45082963 0.51627105 ... 0.63176256 0.43585116 0.49438152]]\n",
      "\n",
      " [[0.46983755 0.38665885 0.49199274 ... 0.3252541  0.4162777  0.5795303 ]\n",
      "  [0.47100127 0.38433838 0.49839115 ... 0.32439727 0.4105     0.583244  ]\n",
      "  [0.46339452 0.39807686 0.49361712 ... 0.32933414 0.41119456 0.57173175]\n",
      "  [0.47056007 0.36773095 0.4997049  ... 0.33063272 0.42213944 0.59269655]\n",
      "  [0.48539254 0.36465412 0.4819067  ... 0.33504188 0.43386447 0.59174836]]\n",
      "\n",
      " [[0.56096977 0.46272102 0.43901137 ... 0.46431577 0.4717019  0.48739335]\n",
      "  [0.55285966 0.45693353 0.42772612 ... 0.4551593  0.47294915 0.48736465]\n",
      "  [0.5493915  0.4631592  0.43838158 ... 0.4532443  0.47215283 0.49127463]\n",
      "  [0.5469779  0.46264046 0.4374392  ... 0.45103943 0.47335663 0.4908478 ]\n",
      "  [0.5549428  0.4602454  0.43327036 ... 0.4572754  0.4694989  0.491188  ]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_seq_length = 5    # Maximum length of the input sequence\n",
    "    d_k = 64                # Dimensionality of the linearly projected queries and keys\n",
    "    d_v = 64                # Dimensionality of the linearly projected values\n",
    "    batch_size = 64         # Batch size from the training process\n",
    "\n",
    "    # Initialize the queries, keys, values randomely\n",
    "    queries = np.random.random((batch_size, input_seq_length, d_k))\n",
    "    keys    = np.random.random((batch_size, input_seq_length, d_k))\n",
    "    values  = np.random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "    # Feed to the attention layer\n",
    "    attention = ScaledDotProductAttentionLayer()\n",
    "    out = attention(queries, keys, values, d_k)\n",
    "\n",
    "    # Report\n",
    "    print(\"Output from the attention layer: \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention class\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the scaled dot product attention layer\n",
    "        self.attention = ScaledDotProductAttentionLayer()\n",
    "\n",
    "        # Initialization\n",
    "        self.heads = h            # Number of attention heads\n",
    "        self.d_k = d_k            # Dimension of the key vector (and also the query vector)\n",
    "        self.d_v = d_v            # Dimension of the value vector\n",
    "        self.d_model = d_model    # Dimension of the model\n",
    "\n",
    "        # Initialize dense layer for learned projection matrix for queries, keys, values, and model\n",
    "        self.W_q = tf.keras.layers.Dense(units=d_k)\n",
    "        self.W_k = tf.keras.layers.Dense(units=d_k)\n",
    "        self.W_v = tf.keras.layers.Dense(units=d_v)\n",
    "        self.W_o = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "\n",
    "    # Function for reshaping the tensor \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "\n",
    "        # If flag is on\n",
    "        # Used when recieving the linearly projected queries, keys, or values as input\n",
    "        # Final shape should be: (batch_size, heads, seq_length, -1)\n",
    "        if flag:\n",
    "\n",
    "            # Reshape the tensor\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], heads, -1))\n",
    "\n",
    "            # Transpose the tensor\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\n",
    "        # If flag is off\n",
    "        # Use after the data feeded into the multi head attention layer\n",
    "        # Final shape should be: (batch_size, seq_length, d_k)\n",
    "        else:\n",
    "\n",
    "            # Transpose\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\n",
    "            # Reshape\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], self.d_k))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    # Call function\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "\n",
    "        # Reshape queries, keys, values to be able to compute all heads in parallel\n",
    "        queries_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        keys_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        values_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        \n",
    "        # Compute multi-head attention\n",
    "        output_reshaped = self.attention(queries_reshaped, keys_reshaped, values_reshaped, self.d_k, mask)\n",
    "\n",
    "        # Rearrange the output into concatenated form\n",
    "        output = self.reshape_tensor(output_reshaped, self.heads, False)\n",
    "\n",
    "        # Apply the linear projection to the output\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the multi-head attention layer:  tf.Tensor(\n",
      "[[[-1.66469619e-01  1.63553625e-01 -1.36685461e-01 ... -5.46096712e-02\n",
      "    2.07701206e-01  4.19217208e-03]\n",
      "  [-1.64007828e-01  1.63331047e-01 -1.39813229e-01 ... -5.35636432e-02\n",
      "    2.07937583e-01  1.66530185e-03]\n",
      "  [-1.64741859e-01  1.65688321e-01 -1.38286382e-01 ... -5.55306152e-02\n",
      "    2.07518727e-01  1.68163725e-03]\n",
      "  [-1.66954830e-01  1.68592140e-01 -1.38605982e-01 ... -5.26690148e-02\n",
      "    2.08170936e-01 -2.56264728e-04]\n",
      "  [-1.68323398e-01  1.66108802e-01 -1.37974724e-01 ... -5.30769862e-02\n",
      "    2.07550332e-01  2.89576175e-03]]\n",
      "\n",
      " [[-2.06464544e-01  1.51354447e-01 -1.83266789e-01 ... -1.02626972e-01\n",
      "    1.95411921e-01  1.12775676e-01]\n",
      "  [-2.11605936e-01  1.46915451e-01 -1.80304363e-01 ... -1.03469551e-01\n",
      "    1.96917787e-01  1.14309050e-01]\n",
      "  [-2.09931299e-01  1.48911923e-01 -1.78512841e-01 ... -1.05035760e-01\n",
      "    1.94795027e-01  1.13725916e-01]\n",
      "  [-2.08803311e-01  1.46946937e-01 -1.78783491e-01 ... -1.03333235e-01\n",
      "    1.93902045e-01  1.13291331e-01]\n",
      "  [-2.07583368e-01  1.49368569e-01 -1.83121383e-01 ... -1.00916035e-01\n",
      "    1.92768961e-01  1.14398278e-01]]\n",
      "\n",
      " [[-7.41645098e-02  2.40965128e-01 -3.05508792e-01 ... -9.33928639e-02\n",
      "    2.74478257e-01 -2.80033182e-02]\n",
      "  [-7.50012025e-02  2.35744104e-01 -3.03340167e-01 ... -9.32131037e-02\n",
      "    2.73188055e-01 -2.62416787e-02]\n",
      "  [-7.06386045e-02  2.41468832e-01 -3.07637185e-01 ... -9.19756964e-02\n",
      "    2.71284789e-01 -2.84473859e-02]\n",
      "  [-7.29981959e-02  2.39626035e-01 -3.05282950e-01 ... -9.21010822e-02\n",
      "    2.73340523e-01 -2.70019528e-02]\n",
      "  [-7.37399161e-02  2.39798874e-01 -3.03258657e-01 ... -9.58323255e-02\n",
      "    2.72573560e-01 -2.80492734e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.59186989e-01  1.63364083e-01 -1.16645329e-01 ... -4.94690128e-02\n",
      "    1.64450869e-01 -3.48396163e-04]\n",
      "  [-1.59334511e-01  1.62710190e-01 -1.14190742e-01 ... -5.17378822e-02\n",
      "    1.64265886e-01 -4.45075892e-03]\n",
      "  [-1.57048747e-01  1.63146392e-01 -1.13984250e-01 ... -4.84385863e-02\n",
      "    1.61892399e-01 -5.36267227e-03]\n",
      "  [-1.58570319e-01  1.61839336e-01 -1.16159476e-01 ... -5.07341400e-02\n",
      "    1.64450318e-01 -4.56389412e-03]\n",
      "  [-1.58715338e-01  1.61721081e-01 -1.13455839e-01 ... -5.28395958e-02\n",
      "    1.58138618e-01 -7.52363994e-04]]\n",
      "\n",
      " [[-1.75889775e-01  2.97498792e-01 -2.09639892e-01 ... -1.40693784e-01\n",
      "    3.21705699e-01 -1.01646818e-02]\n",
      "  [-1.75642177e-01  2.98156887e-01 -2.09872603e-01 ... -1.44112185e-01\n",
      "    3.23261201e-01 -9.89242829e-03]\n",
      "  [-1.77130073e-01  2.97738105e-01 -2.09510401e-01 ... -1.43324256e-01\n",
      "    3.20471197e-01 -9.48069990e-03]\n",
      "  [-1.73790485e-01  2.98160642e-01 -2.09079102e-01 ... -1.40855148e-01\n",
      "    3.23666692e-01 -9.48339701e-03]\n",
      "  [-1.75654247e-01  2.96676159e-01 -2.10745722e-01 ... -1.39267370e-01\n",
      "    3.23177874e-01 -8.72277562e-03]]\n",
      "\n",
      " [[-6.67912439e-02  2.02802867e-01 -1.13499835e-01 ... -1.37314871e-01\n",
      "    1.60205752e-01  2.59655733e-02]\n",
      "  [-6.60207719e-02  2.03240499e-01 -1.12393759e-01 ... -1.37312725e-01\n",
      "    1.60799831e-01  2.54396461e-02]\n",
      "  [-6.81916699e-02  2.00555593e-01 -1.13377668e-01 ... -1.38326228e-01\n",
      "    1.58549517e-01  2.35281922e-02]\n",
      "  [-6.86902478e-02  2.01201454e-01 -1.12650432e-01 ... -1.41055733e-01\n",
      "    1.58339337e-01  2.30395272e-02]\n",
      "  [-6.65023029e-02  2.02725902e-01 -1.12111837e-01 ... -1.38446778e-01\n",
      "    1.61667794e-01  2.75103878e-02]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_seq_length = 5    # Maximum length of the input sequence\n",
    "    h = 8                   # Number of self-attention heads\n",
    "    d_k = 64                # Dimensionality of the linearly projected queries and keys\n",
    "    d_v = 64                # Dimensionality of the linearly projected values\n",
    "    d_model = 512           # Dimensionality of the model sub-layers' outputs\n",
    "    batch_size = 64         # Batch size from the training process\n",
    "\n",
    "    # Initialize the queries, keys, values randomely\n",
    "    queries = np.random.random((batch_size, input_seq_length, d_k))\n",
    "    keys    = np.random.random((batch_size, input_seq_length, d_k))\n",
    "    values  = np.random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "    # Feed to the multi-head attention layer\n",
    "    multihead_attention = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "    out = multihead_attention(queries, keys, values)\n",
    "\n",
    "    # Report\n",
    "    print(\"Output from the multi-head attention layer: \", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### MASKING\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for masking the paddings\n",
    "def padding_mask(inputs):\n",
    "\n",
    "    # Create mask (i.e. marks zero paddings by a 1 and 0 elsewhere)\n",
    "    mask = tf.math.equal(inputs, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lookahead mask\n",
    "def lookahead_mask(shape):\n",
    "\n",
    "    # Create mask (i.e. marks future enteries by a 1 and 0 elsewhere)\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((shape, shape)), num_lower=-1, num_upper=0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  [1 2 3 4 0 0 0]\n",
      "Masked inputs:  tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)\n",
      "Masked lookahead of shape 5:  tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Sample dataset\n",
    "    inputs = np.array([1, 2, 3, 4, 0, 0, 0])\n",
    "    \n",
    "    # Apply padding mask\n",
    "    masked_inputs = padding_mask(inputs)\n",
    "    \n",
    "    # Apply lookahead mask\n",
    "    masked_lookahead = lookahead_mask(shape=5)\n",
    "\n",
    "    # Report\n",
    "    print(\"Inputs: \", inputs)\n",
    "    print(\"Masked inputs: \", masked_inputs)\n",
    "    print(\"Masked lookahead of shape 5: \", masked_lookahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ENCODER\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for Add & Norm layer\n",
    "class AddNormalizationLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the layer normalization layer\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x, sublayer_x):\n",
    "\n",
    "        # Add the sublayer input and output together\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization\n",
    "        out = self.layer_norm(add)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for Feed-Forward layer\n",
    "class FeedForwardLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the dense layers\n",
    "        self.fully_connected_1 = tf.keras.layers.Dense(units=d_ff)\n",
    "        self.fully_connected_2 = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # Initialize the activation function\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x):\n",
    "\n",
    "        # Feed the data\n",
    "        x = self.fully_connected_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fully_connected_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for Transformer Encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialization (to use in rest of the class)\n",
    "        self.sequence_length = sequence_length \n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Attention layer\n",
    "        self.multihead_attention = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "\n",
    "        # Feed-forward layer\n",
    "        self.feed_forward = FeedForwardLayer(d_ff, d_model)\n",
    "\n",
    "        # Add & Norm layer\n",
    "        self.add_norm_1 = AddNormalizationLayer()\n",
    "        self.add_norm_2 = AddNormalizationLayer()\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # Build the model (form model summary)\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "\n",
    "    # Build function\n",
    "    def build_graph(self):\n",
    "\n",
    "        # Construct the model\n",
    "        input_layer = tf.keras.layers.Input(shape=(self.sequence_length, self.d_model))\n",
    "        return tf.keras.Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        multihead_output   = self.multihead_attention(x, x, x, padding_mask)\n",
    "        multihead_output   = self.dropout_1(multihead_output, training=training)\n",
    "        addnorm_output     = self.add_norm_1(x, multihead_output)\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        feedforward_output = self.dropout_2(feedforward_output, training=training)\n",
    "        addnorm_output     = self.add_norm_2(addnorm_output, feedforward_output)\n",
    "\n",
    "        return addnorm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for the full model\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Positional encoding layer\n",
    "        self.positional_encoding = PositionEmbeddingLayerWithFixedWeights(sequence_length, vocab_size, d_model)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # Encoder layers (for N times)\n",
    "        self.encoder_layer = [EncoderLayer(sequence_length, h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    # Call function\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        x = self.positional_encoding(input_sentence)\n",
    "        x = self.dropout(x, training=training) \n",
    "        for i_index, i_layer in enumerate(self.encoder_layer):\n",
    "            x = i_layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_layer_7 (  (None, 64, 512)     131776      ['input_1[0][0]',                \n",
      " MultiHeadAttentionLayer)                                         'input_1[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 64, 512)      0           ['multi_head_attention_layer_7[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " add_normalization_layer_12 (Ad  (None, 64, 512)     1024        ['input_1[0][0]',                \n",
      " dNormalizationLayer)                                             'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " feed_forward_layer_6 (FeedForw  (None, 64, 512)     2099712     ['add_normalization_layer_12[0][0\n",
      " ardLayer)                                                       ]']                              \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 64, 512)      0           ['feed_forward_layer_6[0][0]']   \n",
      "                                                                                                  \n",
      " add_normalization_layer_13 (Ad  (None, 64, 512)     1024        ['add_normalization_layer_12[0][0\n",
      " dNormalizationLayer)                                            ]',                              \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,233,536\n",
      "Trainable params: 2,233,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model summary:  None\n",
      "Output of the encoder: \n",
      " tf.Tensor(\n",
      "[[[ 1.8303759   0.87025183 -0.44571137 ...  1.6054647  -0.98586106\n",
      "   -0.9212881 ]\n",
      "  [ 1.4203045   1.0462345  -0.47421685 ...  2.1323347  -0.4142925\n",
      "   -2.1540172 ]\n",
      "  [ 1.5725088   0.661431    0.03918649 ...  0.620613   -1.1038152\n",
      "   -1.7110561 ]\n",
      "  ...\n",
      "  [ 1.9679447  -0.09403022 -0.6548675  ...  1.8380584   0.18856877\n",
      "   -0.78484774]\n",
      "  [ 0.9894199   0.98310524 -1.0483859  ...  1.7165087   0.75330406\n",
      "   -1.0398772 ]\n",
      "  [ 2.0942085   0.85017645 -1.1279916  ...  1.3656037  -0.27756512\n",
      "   -0.8036723 ]]\n",
      "\n",
      " [[ 1.5414956   0.89080834 -0.6790815  ...  1.3606948  -0.6445713\n",
      "   -1.834032  ]\n",
      "  [ 2.6960359   0.68271196 -0.83825845 ...  2.384902   -0.5958009\n",
      "   -0.68296725]\n",
      "  [ 2.2936938   0.96941704 -0.9461065  ...  1.2795631  -1.6286345\n",
      "   -1.7430954 ]\n",
      "  ...\n",
      "  [ 1.9841108   0.257275   -0.36690548 ...  1.3397292  -0.4275344\n",
      "   -0.25917327]\n",
      "  [ 1.1173277   1.0704408  -0.8790828  ...  1.6447357  -0.06552395\n",
      "   -2.185788  ]\n",
      "  [ 1.6175944   0.64202994 -0.95211893 ...  1.3000655  -0.55930537\n",
      "   -1.1176949 ]]\n",
      "\n",
      " [[ 1.3330264   1.2026951  -1.1130928  ...  1.8264732  -0.66304857\n",
      "   -1.3634169 ]\n",
      "  [ 2.1496813   1.165926    0.0545629  ...  1.9918529   0.1182917\n",
      "   -1.451256  ]\n",
      "  [ 2.7426844   0.75769454 -0.52435946 ...  1.5203083  -0.60601306\n",
      "   -1.536275  ]\n",
      "  ...\n",
      "  [ 1.5078312   0.21817929 -1.0805976  ...  1.4736536  -0.10290859\n",
      "   -1.729275  ]\n",
      "  [ 2.054433    0.29437548 -0.9771863  ...  1.5440116  -0.30796048\n",
      "   -1.6636432 ]\n",
      "  [ 1.4504557   0.91526914  0.3535116  ...  0.77351815  0.09432277\n",
      "   -0.43208563]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.6195667  -0.04378502 -0.57218796 ...  0.64500296  0.47612852\n",
      "   -2.3029313 ]\n",
      "  [ 1.7940413   1.2375264  -0.27812818 ...  0.5571155  -0.06198175\n",
      "   -1.3022414 ]\n",
      "  [ 1.8698981   0.5032857  -0.02608921 ...  1.834605   -0.36794633\n",
      "   -1.357991  ]\n",
      "  ...\n",
      "  [ 1.5296854   0.29807037 -0.50671446 ...  0.6881973  -0.35714233\n",
      "   -1.7830229 ]\n",
      "  [ 1.1294196   1.3196896  -1.0872636  ...  1.4362524   0.33794576\n",
      "   -1.5502597 ]\n",
      "  [ 1.4426197   0.7613129  -0.9581398  ...  1.5398444  -0.65881056\n",
      "   -1.7967036 ]]\n",
      "\n",
      " [[ 1.5244938   1.4303159  -0.6483452  ...  1.995605   -0.13112977\n",
      "   -1.8772528 ]\n",
      "  [ 1.236625    0.9199253  -1.0129108  ...  1.091606   -0.57693845\n",
      "   -1.829667  ]\n",
      "  [ 2.4097035   0.9439798   0.35535595 ...  2.1684792  -0.41532505\n",
      "   -1.9968945 ]\n",
      "  ...\n",
      "  [ 0.92241174  0.3056186  -0.40840277 ...  1.1910135  -1.0266157\n",
      "   -2.117644  ]\n",
      "  [ 1.4294554   1.0951362  -1.566749   ...  1.8734032  -0.6415233\n",
      "   -1.4058759 ]\n",
      "  [ 1.517436    0.4840159  -1.092095   ...  2.0979326   0.00516431\n",
      "   -1.5599612 ]]\n",
      "\n",
      " [[ 2.5034585   0.44902512 -0.51231354 ...  1.5394092  -0.35621047\n",
      "   -2.0028968 ]\n",
      "  [ 0.7987088   1.1525029  -0.04727174 ...  1.3200151  -0.70008844\n",
      "   -1.2589513 ]\n",
      "  [ 1.9497355   0.579839   -0.56161773 ...  1.1647612  -0.8608997\n",
      "   -1.7892926 ]\n",
      "  ...\n",
      "  [ 0.81362677  0.52643085 -0.58775324 ...  1.5233421  -1.7165879\n",
      "   -1.5055056 ]\n",
      "  [ 1.9567822   0.9088896  -0.8678151  ...  1.2333218  -1.3381054\n",
      "   -1.0441155 ]\n",
      "  [ 1.0066751   0.27313372 -0.8253472  ...  0.8735737  -0.60349447\n",
      "   -0.25786796]]], shape=(64, 64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    h = 8                     # Number of self-attention heads\n",
    "    d_k = 64                  # Dimension of the key and query vectors\n",
    "    d_v = 64                  # Dimension of the value vectors\n",
    "    d_ff = 2048               # Dimension of the inner feed-forward layer\n",
    "    d_model = 512             # Dimension of the mode syb-layer' output\n",
    "    n = 6                     # Number of encoder layers\n",
    "    batch_size = 64           # Batch size\n",
    "    dropout_rate = 0.1        # Dropout rate\n",
    "    enc_vocab_size = 8192     # Encoder vocabulary size\n",
    "    input_seq_length = 64     # Maximum length of the input sequence\n",
    "\n",
    "    # Sample input dataset\n",
    "    input_seq = np.random.random((batch_size, input_seq_length))\n",
    "\n",
    "    # Feed to the encoder\n",
    "    encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "    output_encoder = encoder(input_seq, None, True)\n",
    "    \n",
    "    # Model summary\n",
    "    encoder_layer = EncoderLayer(input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "    \n",
    "    # Report\n",
    "    print(\"Model summary: \", encoder_layer.build_graph().summary())\n",
    "    print(\"Output of the encoder: \\n\", output_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### DECODER\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for decoder layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "\n",
    "        # Inherite parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Multi-head attention layer\n",
    "        self.multihead_attention_1 = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "        self.multihead_attention_2 = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "\n",
    "        # Feed-forward layer\n",
    "        self.feed_forward = FeedForwardLayer(d_ff, d_model)\n",
    "\n",
    "        # Add & Norm layer\n",
    "        self.add_norm_1 = AddNormalizationLayer()\n",
    "        self.add_norm_2 = AddNormalizationLayer()\n",
    "        self.add_norm_3 = AddNormalizationLayer()\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # Initialization\n",
    "        self.sequence_length = sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Build the model (for model summary)\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "\n",
    "    # Build function\n",
    "    def build_graph(self):\n",
    "\n",
    "        # Construct the model\n",
    "        input_layer = tf.keras.layers.Input(shape=(self.sequence_length, self.d_model))\n",
    "        return tf.keras.Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        multihead_output_1 = self.multihead_attention_1(x, x, x, lookahead_mask)\n",
    "        multihead_output_1 = self.dropout_1(multihead_output_1, training=training)\n",
    "        addnorm_output_1 = self.add_norm_1(x, multihead_output_1)\n",
    "\n",
    "        multihead_output_2 = self.multihead_attention_2(addnorm_output_1, encoder_output, encoder_output, padding_mask)\n",
    "        multihead_output_2 = self.dropout_2(multihead_output_2, training=training)\n",
    "        addnorm_output_2 = self.add_norm_2(addnorm_output_1, multihead_output_2)\n",
    "\n",
    "        feedforward_output = self.feed_forward(addnorm_output_2)\n",
    "        feedforward_output = self.dropout_3(feedforward_output, training=training)\n",
    "        addnorm_output_3 = self.add_norm_3(addnorm_output_2, feedforward_output)\n",
    "\n",
    "        return addnorm_output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for the full decoder model\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Positional encoding layer\n",
    "        self.pos_encoding = PositionEmbeddingLayerWithFixedWeights(sequence_length, vocab_size, d_model)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # Decoder layers (for N times)\n",
    "        self.decoder_layer = [DecoderLayer(sequence_length, h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    # Call function\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        x = self.pos_encoding(output_target)\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i_index, i_layer in enumerate(self.decoder_layer):\n",
    "            x = i_layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 40, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_layer_20   (None, 40, 512)     131776      ['input_2[0][0]',                \n",
      " (MultiHeadAttentionLayer)                                        'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 40, 512)      0           ['multi_head_attention_layer_20[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " add_normalization_layer_32 (Ad  (None, 40, 512)     1024        ['input_2[0][0]',                \n",
      " dNormalizationLayer)                                             'dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_layer_21   (None, 40, 512)     131776      ['add_normalization_layer_32[0][0\n",
      " (MultiHeadAttentionLayer)                                       ]',                              \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 40, 512)      0           ['multi_head_attention_layer_21[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " add_normalization_layer_33 (Ad  (None, 40, 512)     1024        ['add_normalization_layer_32[0][0\n",
      " dNormalizationLayer)                                            ]',                              \n",
      "                                                                  'dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " feed_forward_layer_13 (FeedFor  (None, 40, 512)     2099712     ['add_normalization_layer_33[0][0\n",
      " wardLayer)                                                      ]']                              \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 40, 512)      0           ['feed_forward_layer_13[0][0]']  \n",
      "                                                                                                  \n",
      " add_normalization_layer_34 (Ad  (None, 40, 512)     1024        ['add_normalization_layer_33[0][0\n",
      " dNormalizationLayer)                                            ]',                              \n",
      "                                                                  'dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,366,336\n",
      "Trainable params: 2,366,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model summary:  None\n",
      "Decoder Output:  tf.Tensor(\n",
      "[[[-1.25222778e+00 -4.59171414e-01 -9.78158340e-02 ... -1.51361048e-01\n",
      "    3.38431716e+00 -1.53314137e+00]\n",
      "  [-1.17075884e+00 -4.91851985e-01  1.68581419e-02 ... -9.03228670e-02\n",
      "    3.32597756e+00 -1.52517009e+00]\n",
      "  [-1.19850326e+00 -5.40853858e-01  5.75497225e-02 ... -1.36776725e-02\n",
      "    3.27406073e+00 -1.58571553e+00]\n",
      "  ...\n",
      "  [-1.22314465e+00 -5.01990914e-01 -3.55105281e-01 ... -1.83043510e-01\n",
      "    3.27372289e+00 -1.61581659e+00]\n",
      "  [-1.11576986e+00 -4.77783054e-01 -3.31649899e-01 ... -2.14990616e-01\n",
      "    3.30621123e+00 -1.59914958e+00]\n",
      "  [-1.00415587e+00 -5.01738906e-01 -2.14702100e-01 ... -2.07547665e-01\n",
      "    3.31667900e+00 -1.55611455e+00]]\n",
      "\n",
      " [[-1.29481936e+00 -4.47887748e-01 -8.54866803e-02 ... -1.91644385e-01\n",
      "    3.33507252e+00 -1.43326700e+00]\n",
      "  [-1.21655738e+00 -4.90338564e-01  2.35127583e-02 ... -1.36004150e-01\n",
      "    3.28626657e+00 -1.43067503e+00]\n",
      "  [-1.23695374e+00 -5.51006556e-01  5.90366758e-02 ... -6.44801781e-02\n",
      "    3.23827434e+00 -1.48096859e+00]\n",
      "  ...\n",
      "  [-1.26963019e+00 -4.90514755e-01 -3.62476140e-01 ... -2.20925391e-01\n",
      "    3.22653794e+00 -1.52013159e+00]\n",
      "  [-1.16025245e+00 -4.62652832e-01 -3.17354977e-01 ... -2.52681375e-01\n",
      "    3.26473284e+00 -1.50438213e+00]\n",
      "  [-1.05982339e+00 -4.89959240e-01 -2.03855574e-01 ... -2.37147868e-01\n",
      "    3.26681805e+00 -1.46970046e+00]]\n",
      "\n",
      " [[-1.32128274e+00 -4.73725379e-01 -7.15680048e-02 ... -2.13908419e-01\n",
      "    3.38899302e+00 -1.48651528e+00]\n",
      "  [-1.24017608e+00 -5.18371165e-01  4.88223210e-02 ... -1.59463331e-01\n",
      "    3.33084345e+00 -1.46033549e+00]\n",
      "  [-1.25589013e+00 -5.68964660e-01  8.47293660e-02 ... -7.26182014e-02\n",
      "    3.27331161e+00 -1.50906706e+00]\n",
      "  ...\n",
      "  [-1.29509354e+00 -5.28542697e-01 -3.12757969e-01 ... -2.29700565e-01\n",
      "    3.24699640e+00 -1.53653061e+00]\n",
      "  [-1.17600214e+00 -5.00583291e-01 -2.95605063e-01 ... -2.59535551e-01\n",
      "    3.27826190e+00 -1.52501702e+00]\n",
      "  [-1.05938482e+00 -5.23974121e-01 -1.95027217e-01 ... -2.46579021e-01\n",
      "    3.28414655e+00 -1.48298025e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.24355543e+00 -5.64423263e-01 -1.54288873e-01 ... -2.05782518e-01\n",
      "    3.36237288e+00 -1.46364641e+00]\n",
      "  [-1.16343164e+00 -6.16504312e-01 -3.80875394e-02 ... -1.45508960e-01\n",
      "    3.30806613e+00 -1.45239913e+00]\n",
      "  [-1.19263017e+00 -6.77011430e-01 -4.33383742e-03 ... -6.41735941e-02\n",
      "    3.25667000e+00 -1.50489271e+00]\n",
      "  ...\n",
      "  [-1.21316743e+00 -6.10386252e-01 -4.19385821e-01 ... -2.38610968e-01\n",
      "    3.25854206e+00 -1.53040457e+00]\n",
      "  [-1.10255659e+00 -5.88291466e-01 -3.85023415e-01 ... -2.62601763e-01\n",
      "    3.28722525e+00 -1.51739192e+00]\n",
      "  [-9.95305300e-01 -6.17111266e-01 -2.79230177e-01 ... -2.46557146e-01\n",
      "    3.29317617e+00 -1.48369026e+00]]\n",
      "\n",
      " [[-1.25838590e+00 -4.70152617e-01 -7.81646445e-02 ... -2.21274853e-01\n",
      "    3.44137859e+00 -1.58694470e+00]\n",
      "  [-1.18050313e+00 -5.14511764e-01  1.96508095e-02 ... -1.54817492e-01\n",
      "    3.38672900e+00 -1.57761180e+00]\n",
      "  [-1.20765853e+00 -5.67093313e-01  4.01550271e-02 ... -7.81184733e-02\n",
      "    3.33367753e+00 -1.63011849e+00]\n",
      "  ...\n",
      "  [-1.25737762e+00 -5.06853878e-01 -3.48161250e-01 ... -2.64406413e-01\n",
      "    3.31915069e+00 -1.66242397e+00]\n",
      "  [-1.14127135e+00 -4.77584779e-01 -3.25961232e-01 ... -2.93203413e-01\n",
      "    3.34458756e+00 -1.65023661e+00]\n",
      "  [-1.03772020e+00 -5.09792984e-01 -2.17446551e-01 ... -2.73288339e-01\n",
      "    3.34538698e+00 -1.61268198e+00]]\n",
      "\n",
      " [[-1.31269228e+00 -5.12669027e-01 -1.61476091e-01 ... -2.16246992e-01\n",
      "    3.38359594e+00 -1.49823773e+00]\n",
      "  [-1.22941780e+00 -5.51061809e-01 -4.83651906e-02 ... -1.55396894e-01\n",
      "    3.32434249e+00 -1.48736179e+00]\n",
      "  [-1.25989795e+00 -6.03302658e-01 -3.04178987e-03 ... -7.71527067e-02\n",
      "    3.26575565e+00 -1.54947150e+00]\n",
      "  ...\n",
      "  [-1.26907599e+00 -5.26865363e-01 -4.30239677e-01 ... -2.46584669e-01\n",
      "    3.26368546e+00 -1.60048223e+00]\n",
      "  [-1.15863025e+00 -4.96564627e-01 -4.00387734e-01 ... -2.68631905e-01\n",
      "    3.29294920e+00 -1.58822560e+00]\n",
      "  [-1.04891443e+00 -5.18708587e-01 -2.90750355e-01 ... -2.56226182e-01\n",
      "    3.30166888e+00 -1.55084848e+00]]], shape=(64, 40, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparameters\n",
    "    h = 8                  # Number of heads\n",
    "    d_k = 64               # Dimension of the key and query vectors\n",
    "    d_v = 64               # Dimension of the value vectors\n",
    "    d_ff = 2048            # Dimension of the feed-forward layer\n",
    "    d_model = 512          # Dimension of the model sub-layer' output\n",
    "    n = 6                  # Number of encoder layers\n",
    "    batch_size = 64        # Batch size\n",
    "    dropout_rate = 0.1     # Dropout rate\n",
    "    dec_vocab_size = 8000  # Decoder vocabulary size\n",
    "    input_seq_length = 40  # Input sequence length\n",
    "\n",
    "    # Sample dataset\n",
    "    input_seq = np.random.random((batch_size, input_seq_length))\n",
    "    encoder_output = np.random.random((batch_size, input_seq_length, d_model))\n",
    "    \n",
    "    # Feed to the decoder\n",
    "    decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "    output_target = decoder(input_seq, encoder_output, None, True)\n",
    "\n",
    "    # Model summary\n",
    "    decoder_layer = DecoderLayer(input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "    \n",
    "    # Print\n",
    "    print(\"Model summary: \", decoder_layer.build_graph().summary())\n",
    "    print(\"Decoder Output: \", output_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### TRANSFORMER\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class for transformer mode\n",
    "class TransformerModel(tf.keras.Model):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Encoder model\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Decoder model\n",
    "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Dense layer\n",
    "        self.last_layer = tf.keras.layers.Dense(dec_vocab_size)\n",
    "\n",
    "    # Function for masking the padding\n",
    "    def padding_mask(self, inputs):\n",
    "\n",
    "        # Create mask (i.e. marks zero paddings by a 1 and 0 elsewhere)\n",
    "        mask = tf.math.equal(inputs, 0)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        # Make the shape broadcastable for the attention weights\n",
    "        mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    # Function for masking the lookahead\n",
    "    def lookahead_mask(self, shape):\n",
    "\n",
    "        # Create mask (i.e. marks future words by a 1 and 0 elsewhere)\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((shape, shape)), -1, 0)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    # Function for calling the model\n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "\n",
    "        # Mask the paddings for input data \n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "\n",
    "        # Mask the lookahead\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask = tf.maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "\n",
    "        # Feed to encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "\n",
    "        # Feed to decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "\n",
    "        # Feed to the last layer\n",
    "        model_output = self.last_layer(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TransformerModel object at 0x7fd8e83aeb20>\n"
     ]
    }
   ],
   "source": [
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Hyperparaneters\n",
    "    enc_vocab_size = 20   # Vocabulary size for the encoder\n",
    "    dec_vocab_size = 20   # Vocabulary size for the decoder\n",
    "    enc_seq_length = 5    # Maximum length of the input sequence\n",
    "    dec_seq_length = 5    # Maximum length of the target sequence\n",
    "    h = 8                 # Number of self-attention heads\n",
    "    d_k = 64              # Dimensionality of the linearly projected queries and keys\n",
    "    d_v = 64              # Dimensionality of the linearly projected values\n",
    "    d_ff = 2048           # Dimensionality of the inner fully connected layer\n",
    "    d_model = 512         # Dimensionality of the model sub-layers' outputs\n",
    "    n = 6                 # Number of layers in the encoder stack\n",
    "    dropout_rate = 0.1    # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "    # Create model\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    print(training_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### TRAINING\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for scheduling the learning ear\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    This class schedules the learning rate.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - d_model (int): the model's dimensionality\n",
    "        - warmup_steps (int): the number of warmup steps\n",
    "        \n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - learning_rate (tf.Tensor): the learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, d_model, warmup_steps=4_000, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initializations\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, step_num):\n",
    "\n",
    "        # Cast step_num to float\n",
    "        step_num = tf.cast(step_num, tf.float32)\n",
    "\n",
    "        # Linearly increase the learning rate for the first warmup_steps times, then decrease it\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        # Learning rate\n",
    "        learning_rate = (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimizer with the learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.Adam(LearningRateScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = PrepareDataset()\n",
    "train_x, train_y, val_x, val_y, train, val, encoder_sequence_length, decoder_sequence_length, encoder_vocabulary_size, decoder_vocabulary_size = dataset(\"./dataset/english-german-both.pkl\")\n",
    "\n",
    "# Convert to tf.data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(encoder_vocabulary_size, decoder_vocabulary_size, encoder_sequence_length, decoder_sequence_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "# Include the metrics monitoring\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.Mean(name=\"train_accuracy\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "\n",
    "# Checkpoint object and manager (for managing multiple checkpoints)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=None)\n",
    "\n",
    "# Initialize lists for stroing the losses\n",
    "train_loss_d = {}\n",
    "val_loss_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the loss\n",
    "def loss_function(target, prediction):\n",
    "    \"\"\"\n",
    "    This function calculates the loss between the target and the prediction.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - target (tf.Tensor): the target tensor\n",
    "        - prediction (tf.Tensor): the prediction tensor\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - loss (tf.Tensor): the loss between the target and the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask the padding values\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Computer the sparse categorical cross entropy loss on the unmasked values\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "\n",
    "    # Calculate the mean loss over the unmasked values\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the accuracy\n",
    "def accuracy_function(target, prediction):\n",
    "    \"\"\"\n",
    "    Function for calculating the accuracy between the target and the prediction.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - target (tf.Tensor): the target tensor\n",
    "        - prediction (tf.Tensor): the prediction tensor\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - out (tf.Tensor): the accuracy between the target and the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask the padding values\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    \n",
    "    # Calculate accuracy and apply the padding mask\n",
    "    accuracy = tf.equal(target, tf.argmax(prediction, axis=2))\n",
    "    accuracy = tf.math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the accuracy from boolean to float32\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "\n",
    "    # Calculate the mean accuracy over the unmasked values\n",
    "    out = tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the training step (to spped up the training process)\n",
    "@tf.function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    \"\"\"\n",
    "    This function performs a training step.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - encoder_input (tf.Tensor): the encoder input\n",
    "        - decoder_input (tf.Tensor): the decoder input\n",
    "        - decoder_output (tf.Tensor): the decoder output\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass (to make predictions)\n",
    "        prediction = model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(decoder_output, prediction)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_function(decoder_output, prediction)\n",
    "\n",
    "    # Fetch the gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "    # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # Update the metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:39:46.811449: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [8000,12]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss = 8.3777, Accuracy = 0.0000\n",
      "Step 50, Loss = 7.6837, Accuracy = 0.1296\n",
      "Step 100, Loss = 7.0861, Accuracy = 0.1743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:41:05.732014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1000,12]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Validation Loss = 5.7459\n",
      "Training Loss = 6.8868, Training Accuracy = 0.1850, Validation Loss = 5.7710\n",
      "Checkpoint saved at ./checkpoints/ckpt-1.\n",
      "Epoch 2/20\n",
      "===========================================\n",
      "Step 0, Loss = 5.8012, Accuracy = 0.2786\n",
      "Step 50, Loss = 5.6080, Accuracy = 0.2747\n",
      "Step 100, Loss = 5.4230, Accuracy = 0.2808\n",
      "Step 0, Validation Loss = 5.0551\n",
      "Training Loss = 5.3390, Training Accuracy = 0.2835, Validation Loss = 4.9494\n",
      "Checkpoint saved at ./checkpoints/ckpt-2.\n",
      "Epoch 3/20\n",
      "===========================================\n",
      "Step 0, Loss = 4.8491, Accuracy = 0.3143\n",
      "Step 50, Loss = 4.7965, Accuracy = 0.3077\n",
      "Step 100, Loss = 4.6796, Accuracy = 0.3217\n",
      "Step 0, Validation Loss = 4.5852\n",
      "Training Loss = 4.6249, Training Accuracy = 0.3284, Validation Loss = 4.3817\n",
      "Checkpoint saved at ./checkpoints/ckpt-3.\n",
      "Epoch 4/20\n",
      "===========================================\n",
      "Step 0, Loss = 4.3557, Accuracy = 0.3571\n",
      "Step 50, Loss = 4.2781, Accuracy = 0.3670\n",
      "Step 100, Loss = 4.1918, Accuracy = 0.3779\n",
      "Step 0, Validation Loss = 4.2525\n",
      "Training Loss = 4.1533, Training Accuracy = 0.3821, Validation Loss = 4.0401\n",
      "Checkpoint saved at ./checkpoints/ckpt-4.\n",
      "Epoch 5/20\n",
      "===========================================\n",
      "Step 0, Loss = 3.9845, Accuracy = 0.4179\n",
      "Step 50, Loss = 3.9057, Accuracy = 0.4117\n",
      "Step 100, Loss = 3.8307, Accuracy = 0.4196\n",
      "Step 0, Validation Loss = 4.0276\n",
      "Training Loss = 3.7993, Training Accuracy = 0.4232, Validation Loss = 3.7782\n",
      "Checkpoint saved at ./checkpoints/ckpt-5.\n",
      "Epoch 6/20\n",
      "===========================================\n",
      "Step 0, Loss = 3.6907, Accuracy = 0.4286\n",
      "Step 50, Loss = 3.5923, Accuracy = 0.4432\n",
      "Step 100, Loss = 3.5289, Accuracy = 0.4495\n",
      "Step 0, Validation Loss = 3.8007\n",
      "Training Loss = 3.5018, Training Accuracy = 0.4530, Validation Loss = 3.5555\n",
      "Checkpoint saved at ./checkpoints/ckpt-6.\n",
      "Epoch 7/20\n",
      "===========================================\n",
      "Step 0, Loss = 3.3717, Accuracy = 0.4643\n",
      "Step 50, Loss = 3.3257, Accuracy = 0.4688\n",
      "Step 100, Loss = 3.2663, Accuracy = 0.4747\n",
      "Step 0, Validation Loss = 3.6389\n",
      "Training Loss = 3.2391, Training Accuracy = 0.4785, Validation Loss = 3.3774\n",
      "Checkpoint saved at ./checkpoints/ckpt-7.\n",
      "Epoch 8/20\n",
      "===========================================\n",
      "Step 0, Loss = 3.0731, Accuracy = 0.4929\n",
      "Step 50, Loss = 3.0901, Accuracy = 0.4931\n",
      "Step 100, Loss = 3.0356, Accuracy = 0.4984\n",
      "Step 0, Validation Loss = 3.4870\n",
      "Training Loss = 3.0105, Training Accuracy = 0.5012, Validation Loss = 3.2350\n",
      "Checkpoint saved at ./checkpoints/ckpt-8.\n",
      "Epoch 9/20\n",
      "===========================================\n",
      "Step 0, Loss = 2.8312, Accuracy = 0.5143\n",
      "Step 50, Loss = 2.8634, Accuracy = 0.5109\n",
      "Step 100, Loss = 2.8020, Accuracy = 0.5183\n",
      "Step 0, Validation Loss = 3.3196\n",
      "Training Loss = 2.7813, Training Accuracy = 0.5207, Validation Loss = 3.0901\n",
      "Checkpoint saved at ./checkpoints/ckpt-9.\n",
      "Epoch 10/20\n",
      "===========================================\n",
      "Step 0, Loss = 2.5782, Accuracy = 0.5464\n",
      "Step 50, Loss = 2.6482, Accuracy = 0.5318\n",
      "Step 100, Loss = 2.6018, Accuracy = 0.5379\n",
      "Step 0, Validation Loss = 3.2265\n",
      "Training Loss = 2.5819, Training Accuracy = 0.5405, Validation Loss = 3.0191\n",
      "Checkpoint saved at ./checkpoints/ckpt-10.\n",
      "Epoch 11/20\n",
      "===========================================\n",
      "Step 0, Loss = 2.3778, Accuracy = 0.5607\n",
      "Step 50, Loss = 2.4496, Accuracy = 0.5539\n",
      "Step 100, Loss = 2.4085, Accuracy = 0.5573\n",
      "Step 0, Validation Loss = 3.1690\n",
      "Training Loss = 2.3921, Training Accuracy = 0.5598, Validation Loss = 2.9228\n",
      "Checkpoint saved at ./checkpoints/ckpt-11.\n",
      "Epoch 12/20\n",
      "===========================================\n",
      "Step 0, Loss = 2.2274, Accuracy = 0.6036\n",
      "Step 50, Loss = 2.2593, Accuracy = 0.5722\n",
      "Step 100, Loss = 2.2230, Accuracy = 0.5756\n",
      "Step 0, Validation Loss = 3.0591\n",
      "Training Loss = 2.2089, Training Accuracy = 0.5776, Validation Loss = 2.8548\n",
      "Checkpoint saved at ./checkpoints/ckpt-12.\n",
      "Epoch 13/20\n",
      "===========================================\n",
      "Step 0, Loss = 2.0339, Accuracy = 0.5929\n",
      "Step 50, Loss = 2.0806, Accuracy = 0.5949\n",
      "Step 100, Loss = 2.0434, Accuracy = 0.5973\n",
      "Step 0, Validation Loss = 3.0527\n",
      "Training Loss = 2.0347, Training Accuracy = 0.5999, Validation Loss = 2.8920\n",
      "Checkpoint saved at ./checkpoints/ckpt-13.\n",
      "Epoch 14/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.9047, Accuracy = 0.6143\n",
      "Step 50, Loss = 1.9072, Accuracy = 0.6147\n",
      "Step 100, Loss = 1.8641, Accuracy = 0.6197\n",
      "Step 0, Validation Loss = 2.9036\n",
      "Training Loss = 1.8518, Training Accuracy = 0.6223, Validation Loss = 2.7270\n",
      "Checkpoint saved at ./checkpoints/ckpt-14.\n",
      "Epoch 15/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.7422, Accuracy = 0.6607\n",
      "Step 50, Loss = 1.7369, Accuracy = 0.6344\n",
      "Step 100, Loss = 1.6934, Accuracy = 0.6427\n",
      "Step 0, Validation Loss = 2.8449\n",
      "Training Loss = 1.6776, Training Accuracy = 0.6458, Validation Loss = 2.6499\n",
      "Checkpoint saved at ./checkpoints/ckpt-15.\n",
      "Epoch 16/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.5576, Accuracy = 0.6714\n",
      "Step 50, Loss = 1.5744, Accuracy = 0.6592\n",
      "Step 100, Loss = 1.5321, Accuracy = 0.6659\n",
      "Step 0, Validation Loss = 2.6985\n",
      "Training Loss = 1.5229, Training Accuracy = 0.6684, Validation Loss = 2.6132\n",
      "Checkpoint saved at ./checkpoints/ckpt-16.\n",
      "Epoch 17/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.4327, Accuracy = 0.6857\n",
      "Step 50, Loss = 1.4394, Accuracy = 0.6770\n",
      "Step 100, Loss = 1.4016, Accuracy = 0.6828\n",
      "Step 0, Validation Loss = 2.7749\n",
      "Training Loss = 1.3917, Training Accuracy = 0.6854, Validation Loss = 2.6234\n",
      "Checkpoint saved at ./checkpoints/ckpt-17.\n",
      "Epoch 18/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.2930, Accuracy = 0.7107\n",
      "Step 50, Loss = 1.3161, Accuracy = 0.6966\n",
      "Step 100, Loss = 1.2686, Accuracy = 0.7052\n",
      "Step 0, Validation Loss = 2.6817\n",
      "Training Loss = 1.2591, Training Accuracy = 0.7068, Validation Loss = 2.5732\n",
      "Checkpoint saved at ./checkpoints/ckpt-18.\n",
      "Epoch 19/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.1150, Accuracy = 0.7607\n",
      "Step 50, Loss = 1.1954, Accuracy = 0.7148\n",
      "Step 100, Loss = 1.1386, Accuracy = 0.7274\n",
      "Step 0, Validation Loss = 2.6594\n",
      "Training Loss = 1.1300, Training Accuracy = 0.7287, Validation Loss = 2.5493\n",
      "Checkpoint saved at ./checkpoints/ckpt-19.\n",
      "Epoch 20/20\n",
      "===========================================\n",
      "Step 0, Loss = 1.0174, Accuracy = 0.7357\n",
      "Step 50, Loss = 1.0643, Accuracy = 0.7404\n",
      "Step 100, Loss = 1.0116, Accuracy = 0.7517\n",
      "Step 0, Validation Loss = 2.6622\n",
      "Training Loss = 1.0059, Training Accuracy = 0.7526, Validation Loss = 2.5454\n",
      "Checkpoint saved at ./checkpoints/ckpt-20.\n"
     ]
    }
   ],
   "source": [
    "# Loop over the epochs\n",
    "for i_epoch in range(epochs):\n",
    "\n",
    "    # Reset the metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "\n",
    "    # Report \n",
    "    print(f\"Epoch {i_epoch + 1}/{epochs}\" + \"\\n===========================================\")\n",
    "\n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for i_step, (train_batch_x, train_batch_y) in enumerate(train_dataset):\n",
    "\n",
    "        # Define the encoder/decoder input/output\n",
    "        encoder_input = train_batch_x[:, 1:]\n",
    "        decoder_input = train_batch_y[:, :-1]\n",
    "        decoder_output = train_batch_y[:, 1:]\n",
    "\n",
    "        # Perform one training step\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        # Report\n",
    "        if (i_step % 50 == 0):\n",
    "            print(f\"Step {i_step}, Loss = {train_loss.result():.4f}, Accuracy = {train_accuracy.result():.4f}\")\n",
    "\n",
    "    # Loop over the validation batches\n",
    "    for i_step, (val_batch_x, val_batch_y) in enumerate(val_dataset):\n",
    "\n",
    "        # Define the encoder/decoder input/output\n",
    "        encoder_input = val_batch_x[:, 1:]\n",
    "        decoder_input = val_batch_y[:, :-1]\n",
    "        decoder_output = val_batch_y[:, 1:]\n",
    "\n",
    "        # Forward pass (to make predictions)\n",
    "        prediction = model(encoder_input, decoder_input, training=False)\n",
    "\n",
    "        # Calculate the loass\n",
    "        loss = loss_function(decoder_output, prediction)\n",
    "\n",
    "        # Update the metrics\n",
    "        val_loss(loss)\n",
    "\n",
    "        # Reoirt\n",
    "        if (i_step % 50 == 0):\n",
    "            print(f\"Step {i_step}, Validation Loss = {val_loss.result():.4f}\")\n",
    "\n",
    "\n",
    "    # Report\n",
    "    print(f\"Training Loss = {train_loss.result():.4f}, Training Accuracy = {train_accuracy.result():.4f}, Validation Loss = {val_loss.result():.4f}\")\n",
    "\n",
    "    # Save the checkpoint after each epoch\n",
    "    if (i_epoch+1) % 1 == 0:\n",
    "\n",
    "        # Save the checkpoint\n",
    "        save_path = checkpoint_manager.save()\n",
    "\n",
    "        # Report\n",
    "        print(f\"Checkpoint saved at {save_path}.\")\n",
    "\n",
    "        # Save the weights\n",
    "        model.save_weights(\"weights/wghts\" + str(i_epoch + 1) + \".ckpt\")\n",
    "\n",
    "        # Report\n",
    "        train_loss_d[i_epoch] = train_loss.result()\n",
    "        val_loss_d[i_epoch] = val_loss.result()\n",
    "\n",
    "# Save the loss values\n",
    "with open(\"./train_loss.pkl\", \"wb\") as file:  pickle.dump(train_loss_d, file)\n",
    "with open(\"./val_loss.pkl\", \"wb\") as file:  pickle.dump(val_loss_d, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################\n",
    "#    VISUALIZATION    #\n",
    "#######################\n",
    "\n",
    "# Load the training and validation loss dictionaries\n",
    "train_loss = pickle.load(open('train_loss.pkl', 'rb'))\n",
    "val_loss = pickle.load(open('val_loss.pkl', 'rb'))\n",
    "\n",
    "# Retrieve each dictionary's values\n",
    "train_values = train_loss.values()\n",
    "val_values = val_loss.values()\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, 21)\n",
    "\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_values, label='Training Loss')\n",
    "plt.plot(epochs, val_values, label='Validation Loss')\n",
    "\n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, 21, 2))\n",
    "\n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### INFERENCE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class for inference\n",
    "class Inference(tf.Module):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, inferencing_model, **kwargs):\n",
    "        \n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the model\n",
    "        self.transformer = inferencing_model\n",
    "\n",
    "    # Function for loading the tokenizer\n",
    "    def load_tokenizer(self, name):\n",
    "\n",
    "        # Load the tokenizer from the specified file\n",
    "        with open(name, 'rb') as handle: return pickle.load(handle)\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, sentence):\n",
    "\n",
    "        # Append START and EOS tokens to the input sentence\n",
    "        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n",
    "\n",
    "        # Load tokenizers for encoder and decoder\n",
    "        enc_tokenizer = self.load_tokenizer('./saved/encoder_tokenizer.pkl')\n",
    "        dec_tokenizer = self.load_tokenizer('./saved/decoder_tokenizer.pkl')\n",
    "\n",
    "        # Encoder input; tokenizing, padding and converting to tensor\n",
    "        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n",
    "        encoder_input = tf.keras.preprocessing.sequence.pad_sequences(encoder_input, maxlen=enc_seq_length, padding='post')\n",
    "        encoder_input = tf.convert_to_tensor(encoder_input, dtype=tf.int64)\n",
    "\n",
    "        # Start of the output sequence is with the <START> token\n",
    "        output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])            # Convert to integers\n",
    "        output_start = tf.convert_to_tensor(output_start[0], dtype=tf.int64)    # Convert to tensor\n",
    "\n",
    "        # End of the output sequence is with the <EOS> token (for breaking the loop)\n",
    "        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])               # Convert to integers\n",
    "        output_end = tf.convert_to_tensor(output_end[0], dtype=tf.int64)      # Convert to tensor\n",
    "\n",
    "        # Output array for storing the predicted tokens (with dynamic size)\n",
    "        decoder_output = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        decoder_output = decoder_output.write(0, output_start)\n",
    "\n",
    "        # Loop over the decoder sequence length\n",
    "        for i in range(dec_seq_length):\n",
    "\n",
    "            # Prediction\n",
    "            prediction = self.transformer(encoder_input,tf. transpose(decoder_output.stack()), training=False)\n",
    "            \n",
    "            # Select the last predicted token\n",
    "            prediction = prediction[:, -1, :]\n",
    "\n",
    "            # Select the prediction with the highest score\n",
    "            predicted_id = tf.argmax(prediction, axis=-1)\n",
    "            predicted_id = predicted_id[0][tf.newaxis]\n",
    "\n",
    "            # Write the selected prediction to the output array at the next available index\n",
    "            decoder_output = decoder_output.write(i + 1, predicted_id)\n",
    "\n",
    "            # Break if an <EOS> token is predicted\n",
    "            if predicted_id == output_end: break\n",
    "\n",
    "        # Transpose the output array and convert to numpy array\n",
    "        output = tf.transpose(decoder_output.stack())[0]\n",
    "        output = output.numpy()\n",
    "\n",
    "        # Initialize an empty list for storing the output string\n",
    "        output_str = []\n",
    "\n",
    "        ### Decode the predicted tokens into an output string\n",
    "\n",
    "        # Loop over the output array\n",
    "        for i in range(output.shape[0]):\n",
    "\n",
    "            # Select the token at the current index\n",
    "            key = output[i]\n",
    "\n",
    "            # Append the token to the output string\n",
    "            output_str.append(dec_tokenizer.index_word[key])\n",
    "\n",
    "        return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:   ich bin durstig \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEST THE CODE\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Dataset parameters\n",
    "    enc_seq_length = encoder_sequence_length      # Encoder sequence length\n",
    "    dec_seq_length = decoder_sequence_length      # Decoder sequence length\n",
    "    enc_vocab_size = encoder_vocabulary_size      # Encoder vocabulary size\n",
    "    dec_vocab_size = decoder_vocabulary_size      # Decoder vocabulary size\n",
    "\n",
    "    # Sample dataset    \n",
    "    sentence = ['im thirsty']                                  # Sentence to translate\n",
    "\n",
    "    # Initialize the model\n",
    "    inferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)\n",
    "\n",
    "    # Load the weights \n",
    "    inferencing_model.load_weights('weights/wghts20.ckpt')      # Load the trained model's weights at the specified epoch\n",
    "\n",
    "    # Inference\n",
    "    translator = Inference(inferencing_model)                  # Create a new instance of the 'Translate' class\n",
    "    translation = translator(sentence)\n",
    "\n",
    "    # Report\n",
    "    print(\"Translation: \", \" \".join(translation).replace(\"start\", \"\").replace(\"eos\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
