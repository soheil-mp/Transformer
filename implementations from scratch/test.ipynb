{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn off the tensorflow loggings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Initialization\n",
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "output_length = 6\n",
    "\n",
    "\n",
    "############################\n",
    "#    TEXT VECTORIZATION    #\n",
    "############################\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "\n",
    "# Convert to tf.data\n",
    "sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# Convert to tensors\n",
    "word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "\n",
    "# Create text vectorizer (for preprocessing)\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length,\n",
    "                                                    max_tokens=vocab_size)\n",
    "# Train the layer\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "# Preprocess the data\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "# Report\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", vectorized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "output_sequence_length = 5\n",
    "vocab_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I am a robot'], ['you too robot']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(1,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to tf.data\n",
    "sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "sentence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=string, numpy=\n",
       "array([[b'I am a robot'],\n",
       "       [b'you too robot']], dtype=object)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "word_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n",
      "Vectorized words:  tf.Tensor(\n",
      "[[5 6 7 2 0]\n",
      " [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Create text vectorizer (for preprocessing)\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length,\n",
    "                                                    max_tokens=vocab_size)\n",
    "# Train the layer\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "# Preprocess the data\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "# Report\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#    WORD EMBEDDING LAYER    #\n",
    "##############################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "output_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded words:  tf.Tensor(\n",
      "[[[-0.01089402  0.01386965  0.04864142  0.01684561  0.03848663\n",
      "    0.04736397]\n",
      "  [ 0.00924904  0.00032661  0.03953311 -0.00486473 -0.04105572\n",
      "   -0.02126145]\n",
      "  [ 0.00253134 -0.03019602 -0.01808629 -0.01031756  0.02852184\n",
      "    0.00721966]\n",
      "  [-0.0281015  -0.04744308 -0.04805942 -0.03590043  0.00918899\n",
      "   -0.03965997]\n",
      "  [-0.04285062 -0.04663532 -0.02162063  0.02399271  0.0155862\n",
      "   -0.03969042]]\n",
      "\n",
      " [[ 0.01689786 -0.01927587  0.00308625 -0.04438348  0.04125218\n",
      "    0.00348777]\n",
      "  [ 0.0100126   0.01868899  0.00589956 -0.03106445  0.03364502\n",
      "    0.01358403]\n",
      "  [-0.0281015  -0.04744308 -0.04805942 -0.03590043  0.00918899\n",
      "   -0.03965997]\n",
      "  [-0.04285062 -0.04663532 -0.02162063  0.02399271  0.0155862\n",
      "   -0.03969042]\n",
      "  [-0.04285062 -0.04663532 -0.02162063  0.02399271  0.0155862\n",
      "   -0.03969042]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "word_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=output_length)\n",
    "\n",
    "# Feed the data\n",
    "embedded_words = word_embedding_layer(vectorized_words)\n",
    "\n",
    "print(\"Embedded words: \", embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#    POSITION EMBEDDING LAYER    #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded indices:  tf.Tensor(\n",
      "[[ 0.00608952  0.00206887 -0.03670409  0.03553356  0.03877778 -0.04307035]\n",
      " [ 0.04962518  0.04153587 -0.00615101 -0.04446935  0.00044483  0.00444321]\n",
      " [ 0.04820463  0.01890317  0.03044811 -0.01812079 -0.04739708  0.01327846]\n",
      " [-0.0372933  -0.01662916  0.03327545 -0.01628338 -0.0472406  -0.01894217]\n",
      " [ 0.02533323  0.04628959  0.03844256  0.02668405  0.03564659 -0.03734051]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "position_embedding_layer = tf.keras.layers.Embedding(input_dim=output_sequence_length, output_dim=output_length)\n",
    "\n",
    "# Initialize the positions\n",
    "position_indices = tf.range(output_sequence_length)\n",
    "\n",
    "# Feed the positions\n",
    "embedded_indices = position_embedding_layer(position_indices)\n",
    "\n",
    "print(\"Embedded indices: \", embedded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#    FINAL EMBEDDING    #\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output embedding:  tf.Tensor(\n",
      "[[[-0.0048045   0.01593852  0.01193733  0.05237917  0.07726441\n",
      "    0.00429362]\n",
      "  [ 0.05887423  0.04186248  0.0333821  -0.04933408 -0.04061089\n",
      "   -0.01681825]\n",
      "  [ 0.05073597 -0.01129285  0.01236182 -0.02843835 -0.01887524\n",
      "    0.02049812]\n",
      "  [-0.0653948  -0.06407224 -0.01478396 -0.05218381 -0.03805162\n",
      "   -0.05860213]\n",
      "  [-0.0175174  -0.00034573  0.01682193  0.05067676  0.05123279\n",
      "   -0.07703093]]\n",
      "\n",
      " [[ 0.02298738 -0.017207   -0.03361784 -0.00884992  0.08002996\n",
      "   -0.03958259]\n",
      "  [ 0.05963779  0.06022485 -0.00025145 -0.07553379  0.03408985\n",
      "    0.01802723]\n",
      "  [ 0.02010312 -0.02853991 -0.0176113  -0.05402121 -0.03820809\n",
      "   -0.02638151]\n",
      "  [-0.08014393 -0.06326447  0.01165482  0.00770932 -0.0316544\n",
      "   -0.05863259]\n",
      "  [-0.0175174  -0.00034573  0.01682193  0.05067676  0.05123279\n",
      "   -0.07703093]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Sum up the word and position embedding\n",
    "final_output_embedding  = embedded_words + embedded_indices\n",
    "\n",
    "print(\"Final output embedding: \", final_output_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#    EMBEDDING LAYER CLASS    #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POsition embedding layer\n",
    "class PositionEmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.word_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=output_dim)\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=output_dim)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Initialize the positions\n",
    "        position_indices = tf.range(start=0, limit=tf.shape(inputs)[-1])\n",
    "\n",
    "        # Feed words and positions to embedding layer\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_positions = self.position_embedding_layer(position_indices)\n",
    "\n",
    "        # Sum up the embeddings\n",
    "        out = embedded_words + embedded_positions\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded layer output:  tf.Tensor(\n",
      "[[[-0.0048573   0.01108037 -0.00096008  0.00831679 -0.07041946\n",
      "   -0.00099148]\n",
      "  [-0.04668096 -0.04576692  0.04594807 -0.0551206  -0.07384367\n",
      "    0.00989917]\n",
      "  [ 0.00748022  0.04786074 -0.07845758  0.0040458  -0.04062707\n",
      "    0.03330592]\n",
      "  [-0.01791889  0.03598766 -0.0087663  -0.01792306  0.0187975\n",
      "    0.03526177]\n",
      "  [ 0.00701702 -0.06934252  0.03205752 -0.0727486   0.06291012\n",
      "   -0.00442809]]\n",
      "\n",
      " [[-0.007153   -0.02741077 -0.06575882 -0.00222047 -0.02321532\n",
      "    0.00115896]\n",
      "  [-0.01011484  0.00387889  0.02005375  0.00350453 -0.0184971\n",
      "    0.0601273 ]\n",
      "  [-0.00112214  0.05705871 -0.07803564  0.00512144 -0.05553848\n",
      "    0.00906985]\n",
      "  [ 0.04086523 -0.01879575  0.04493554 -0.07944542  0.06024898\n",
      "   -0.00207628]\n",
      "  [ 0.00701702 -0.06934252  0.03205752 -0.0727486   0.06291012\n",
      "   -0.00442809]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom layer\n",
    "custom_embedding_layer = PositionEmbeddingLayer(seq_length=output_sequence_length,\n",
    "                                                vocab_size=vocab_size,\n",
    "                                                output_dim=output_length)\n",
    "\n",
    "# Feed the data\n",
    "embedded_layer_output = custom_embedding_layer(vectorized_words)\n",
    "\n",
    "print(\"Embedded layer output: \", embedded_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#    POSITIONAL EMBEDDING LAYER WITH SINE/COSINE    #\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n",
      "Vectorized words:  tf.Tensor(\n",
      "[[5 6 7 2 0]\n",
      " [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Turn off the tensorflow loggings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialization\n",
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "output_length = 6\n",
    "\n",
    "\n",
    "############################\n",
    "#    TEXT VECTORIZATION    #\n",
    "############################\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "\n",
    "# Convert to tf.data\n",
    "sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# Convert to tensors\n",
    "word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "\n",
    "# Create text vectorizer (for preprocessing)\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length,\n",
    "                                                    max_tokens=vocab_size)\n",
    "# Train the layer\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "# Preprocess the data\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "\n",
    "# Report\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", vectorized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded layer output:  tf.Tensor(\n",
      "[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n",
      "    1.9999421 ]\n",
      "  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n",
      "    1.9999142 ]\n",
      "  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n",
      "    1.999877  ]\n",
      "  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n",
      "    1.9999698 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]\n",
      "\n",
      " [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n",
      "    1.9999605 ]\n",
      "  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n",
      "    1.9999814 ]\n",
      "  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#    POSITIONAL ENCODING LAYER    #\n",
    "###################################\n",
    "\n",
    "# POsition embedding layer\n",
    "class PositionEmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, seq_length, vocab_size, output_dim, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.word_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=output_dim)\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=output_dim)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Initialize the positions\n",
    "        position_indices = tf.range(start=0, limit=tf.shape(inputs)[-1])\n",
    "\n",
    "        # Feed words and positions to embedding layer\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_positions = self.position_embedding_layer(position_indices)\n",
    "\n",
    "        # Sum up the embeddings\n",
    "        out = embedded_words + embedded_positions\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Initialize the layer\n",
    "custom_embedding_layer = PositionEmbeddingLayer(seq_length=output_sequence_length,\n",
    "                                                vocab_size=vocab_size,\n",
    "                                                output_dim=output_length\n",
    "                                                )\n",
    "\n",
    "# Feed the data\n",
    "embedded_layer_output = custom_embedding_layer(vectorized_words)\n",
    "\n",
    "print(\"Embedded layer output: \", embedded_layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#    SCALED DOT PRODUCCT ATTENTION    #\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:  tf.Tensor(\n",
      "[[[0.42552757 0.4209954  0.44070917 ... 0.44263887 0.71424043 0.6253689 ]\n",
      "  [0.40977898 0.3985669  0.44573477 ... 0.48359036 0.7332069  0.58461004]\n",
      "  [0.4088182  0.40428117 0.45158815 ... 0.48357344 0.7292079  0.58147603]\n",
      "  [0.41378352 0.4027637  0.44322675 ... 0.5019091  0.7352413  0.56602585]\n",
      "  [0.41841158 0.4188102  0.44571918 ... 0.45981276 0.71746486 0.60514796]]\n",
      "\n",
      " [[0.5964128  0.59765047 0.69386137 ... 0.53982836 0.6785065  0.6883336 ]\n",
      "  [0.6070596  0.6019193  0.6729281  ... 0.528684   0.66200775 0.7016909 ]\n",
      "  [0.6027177  0.5892477  0.6794846  ... 0.53259146 0.6672454  0.6915069 ]\n",
      "  [0.59733444 0.59044796 0.6914817  ... 0.5307452  0.6742085  0.6903019 ]\n",
      "  [0.6056535  0.5855043  0.67923135 ... 0.5180448  0.66661626 0.68405515]]\n",
      "\n",
      " [[0.60367906 0.52061844 0.34178936 ... 0.38859078 0.62887084 0.6432541 ]\n",
      "  [0.58475596 0.5131419  0.34162548 ... 0.38594082 0.6261367  0.6466565 ]\n",
      "  [0.5915015  0.51155126 0.35172242 ... 0.37740138 0.6168307  0.64545625]\n",
      "  [0.59058297 0.51080066 0.34683436 ... 0.39315042 0.62869287 0.6445107 ]\n",
      "  [0.6054493  0.5034542  0.35575068 ... 0.36927256 0.6085393  0.63877666]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.44118005 0.6528061  0.5614681  ... 0.3713206  0.60863686 0.4610855 ]\n",
      "  [0.45830804 0.6743183  0.5702493  ... 0.38991696 0.5909121  0.495098  ]\n",
      "  [0.47031066 0.67762214 0.55342996 ... 0.38020158 0.56197953 0.4827463 ]\n",
      "  [0.45994198 0.6754285  0.56840485 ... 0.38729388 0.57835346 0.48400313]\n",
      "  [0.46544534 0.6763273  0.56046915 ... 0.38668236 0.57790446 0.4928977 ]]\n",
      "\n",
      " [[0.3279381  0.5019955  0.47846788 ... 0.46096483 0.4692459  0.64262533]\n",
      "  [0.32352847 0.48643604 0.48347577 ... 0.46614495 0.4707199  0.6517932 ]\n",
      "  [0.31906    0.48286006 0.49794668 ... 0.46495903 0.4749266  0.66438764]\n",
      "  [0.32550913 0.500981   0.5022674  ... 0.46893355 0.4617015  0.66373795]\n",
      "  [0.32552588 0.48400652 0.49410537 ... 0.48084998 0.4557885  0.66365546]]\n",
      "\n",
      " [[0.5230174  0.5663203  0.41091317 ... 0.24146482 0.41084546 0.5651474 ]\n",
      "  [0.5116624  0.58070886 0.38841254 ... 0.24355459 0.37312824 0.56707406]\n",
      "  [0.525753   0.5618851  0.41023502 ... 0.24008977 0.41260442 0.56929255]\n",
      "  [0.5152373  0.55820143 0.40335402 ... 0.23629469 0.41666833 0.5639149 ]\n",
      "  [0.5296563  0.5841417  0.4045087  ... 0.2422426  0.3915211  0.56112874]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Initialization\n",
    "d_k = 64               # Dimension of the key vector (and also the query vector)\n",
    "d_v = 64               # Dimension of the value vector\n",
    "batch_size = 64\n",
    "input_seq_length = 5   # Maximum length of the input sequence\n",
    "\n",
    "# Scaled dot product class\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "\n",
    "        # Attention socre\n",
    "        attention_scores = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt( tf.cast(d_k, tf.float32) )\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "\n",
    "            # Apply mask\n",
    "            attention_scores += -1e9 * mask\n",
    "\n",
    "        # Apply softmax\n",
    "        weights = tf.keras.backend.softmax(attention_scores)\n",
    "\n",
    "        # Calculate the weighted sum of the value vectors\n",
    "        out = tf.matmul(weights, values)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Initialize the queries, keys, and values\n",
    "queries = np.random.random((batch_size, input_seq_length, d_k))\n",
    "keys = np.random.random((batch_size, input_seq_length, d_k))\n",
    "values = np.random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "# Initialize the layer\n",
    "attention_layer = ScaledDotProductAttention()\n",
    "\n",
    "# Feed the data\n",
    "attention_output = attention_layer(queries, keys, values, d_k)\n",
    "\n",
    "print(\"Attention output: \", attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#    MULTI-HEAD ATTENTION    #\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the multi-head attention: \n",
      " tf.Tensor(\n",
      "[[[-0.08046755 -0.280252   -0.19821672 ... -0.47180074  0.02163792\n",
      "   -0.10966138]\n",
      "  [-0.08029709 -0.27922976 -0.19712687 ... -0.47145772  0.02097024\n",
      "   -0.11003992]\n",
      "  [-0.07974803 -0.27940008 -0.19841315 ... -0.47166383  0.02262438\n",
      "   -0.11007026]\n",
      "  ...\n",
      "  [-0.08090815 -0.28016812 -0.1984467  ... -0.47133493  0.02119658\n",
      "   -0.10933197]\n",
      "  [-0.08028192 -0.27965182 -0.19864747 ... -0.47178677  0.02115291\n",
      "   -0.11039263]\n",
      "  [-0.08022219 -0.27961904 -0.19810556 ... -0.47198555  0.02190784\n",
      "   -0.11036518]]\n",
      "\n",
      " [[-0.08683407 -0.2937707  -0.19879058 ... -0.4523206   0.00739739\n",
      "   -0.08516073]\n",
      "  [-0.08527576 -0.29362044 -0.19832356 ... -0.45231304  0.00636609\n",
      "   -0.08442787]\n",
      "  [-0.08668507 -0.2950291  -0.19909163 ... -0.4523024   0.00647642\n",
      "   -0.08469182]\n",
      "  ...\n",
      "  [-0.0869846  -0.29458934 -0.19853981 ... -0.45199537  0.00694377\n",
      "   -0.08457109]\n",
      "  [-0.08712101 -0.29391465 -0.19853598 ... -0.45353624  0.00666998\n",
      "   -0.08377881]\n",
      "  [-0.08585763 -0.29481712 -0.19821924 ... -0.4521862   0.00650477\n",
      "   -0.0842559 ]]\n",
      "\n",
      " [[-0.11933507 -0.28076723 -0.20805359 ... -0.46023908 -0.00529495\n",
      "   -0.04109037]\n",
      "  [-0.11899348 -0.28023303 -0.20828551 ... -0.46001825 -0.00610678\n",
      "   -0.04129685]\n",
      "  [-0.11950137 -0.28104147 -0.2076389  ... -0.45978805 -0.00453576\n",
      "   -0.04103828]\n",
      "  ...\n",
      "  [-0.11865275 -0.28039557 -0.20859104 ... -0.45965862 -0.00566515\n",
      "   -0.04111891]\n",
      "  [-0.11938518 -0.28042197 -0.20732988 ... -0.46027216 -0.00482151\n",
      "   -0.04064591]\n",
      "  [-0.11894824 -0.28128776 -0.2074618  ... -0.45902827 -0.00523912\n",
      "   -0.04143552]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.09701367 -0.2629303  -0.23329355 ... -0.47876933  0.00458244\n",
      "   -0.05811816]\n",
      "  [-0.09641261 -0.2629044  -0.23336433 ... -0.47934285  0.00499259\n",
      "   -0.05714453]\n",
      "  [-0.09622949 -0.26270625 -0.23378809 ... -0.4794146   0.0047501\n",
      "   -0.05785135]\n",
      "  ...\n",
      "  [-0.09717033 -0.26284644 -0.23396856 ... -0.47833073  0.00423583\n",
      "   -0.05781362]\n",
      "  [-0.09792143 -0.2628211  -0.23320113 ... -0.47912312  0.00452046\n",
      "   -0.05764043]\n",
      "  [-0.09679703 -0.26237154 -0.23298788 ... -0.47936493  0.00466992\n",
      "   -0.05798087]]\n",
      "\n",
      " [[-0.1219766  -0.28631324 -0.17718706 ... -0.46167395 -0.0079734\n",
      "   -0.06291506]\n",
      "  [-0.12165631 -0.28611386 -0.17601798 ... -0.46118045 -0.00687794\n",
      "   -0.06379764]\n",
      "  [-0.12201352 -0.28643614 -0.17621487 ... -0.46166214 -0.00695685\n",
      "   -0.06392797]\n",
      "  ...\n",
      "  [-0.12142901 -0.28663918 -0.17670365 ... -0.46158054 -0.00815152\n",
      "   -0.06312923]\n",
      "  [-0.12289196 -0.28706357 -0.17655031 ... -0.46216732 -0.0067394\n",
      "   -0.06370216]\n",
      "  [-0.12074648 -0.2865103  -0.17699292 ... -0.4613169  -0.00739535\n",
      "   -0.0637683 ]]\n",
      "\n",
      " [[-0.04690984 -0.28147253 -0.18932255 ... -0.4571971   0.01293509\n",
      "   -0.07445034]\n",
      "  [-0.04801666 -0.28121695 -0.18849596 ... -0.45732728  0.01327825\n",
      "   -0.07330984]\n",
      "  [-0.04633599 -0.28102103 -0.18925375 ... -0.45717347  0.01340141\n",
      "   -0.07410134]\n",
      "  ...\n",
      "  [-0.04636353 -0.28129318 -0.18879403 ... -0.45709556  0.01366026\n",
      "   -0.07408133]\n",
      "  [-0.04609293 -0.28142226 -0.1893891  ... -0.45767176  0.01405538\n",
      "   -0.07255816]\n",
      "  [-0.04749656 -0.28096932 -0.18962292 ... -0.4570233   0.01257139\n",
      "   -0.07325183]]], shape=(64, 64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Turn off the tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Scaled dot product attention class\n",
    "class ScaledDotProductAttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    # Call function\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "\n",
    "        # Attention score\n",
    "        attention_score = tf.matmul(queries, keys, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_score += (-1e9 * mask)\n",
    "\n",
    "        # Apply softmax\n",
    "        weights = tf.keras.backend.softmax(attention_score)\n",
    "\n",
    "        # Calculate the weighted sum \n",
    "        out = tf.matmul(weights, values)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Multi-head attention class\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the scaled dot product attention layer\n",
    "        self.attention = ScaledDotProductAttentionLayer()\n",
    "\n",
    "        # Initialization\n",
    "        self.heads = h            # Number of attention heads\n",
    "        self.d_k = d_k            # Dimension of the key vector (and also the query vector)\n",
    "        self.d_v = d_v            # Dimension of the value vector\n",
    "        self.d_model = d_model    # Dimension of the model\n",
    "\n",
    "        # Initialize dense layer for learned projection matrix for queries, keys, values, and model\n",
    "        self.W_q = tf.keras.layers.Dense(units=d_k)\n",
    "        self.W_k = tf.keras.layers.Dense(units=d_k)\n",
    "        self.W_v = tf.keras.layers.Dense(units=d_v)\n",
    "        self.W_o = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "\n",
    "    # Function for reshaping the tensor \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "\n",
    "        # If flag is on\n",
    "        # Used when recieving the linearly projected queries, keys, or values as input\n",
    "        # Final shape should be: (batch_size, heads, seq_length, -1)\n",
    "        if flag:\n",
    "\n",
    "            # Reshape the tensor\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], heads, -1))\n",
    "\n",
    "            # Transpose the tensor\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\n",
    "        # If flag is off\n",
    "        # Use after the data feeded into the multi head attention layer\n",
    "        # Final shape should be: (batch_size, seq_length, d_k)\n",
    "        else:\n",
    "\n",
    "            # Transpose\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "\n",
    "            # Reshape\n",
    "            x = tf.reshape(x, shape=(tf.shape(x)[0], tf.shape(x)[1], self.d_k))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    # Call function\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "\n",
    "        # Reshape queries, keys, values to be able to compute all heads in parallel\n",
    "        queries_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        keys_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        values_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        \n",
    "        # Compute multi-head attention\n",
    "        output_reshaped = self.attention(queries_reshaped, keys_reshaped, values_reshaped, self.d_k, mask)\n",
    "\n",
    "        # Rearrange the output into concatenated form\n",
    "        output = self.reshape_tensor(output_reshaped, self.heads, False)\n",
    "\n",
    "        # Apply the linear projection to the output\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Initialization\n",
    "h = 8                     # Number of self-attention heads\n",
    "d_k = 64                  #  \n",
    "d_v = 64                  #\n",
    "d_model = 512             #\n",
    "batch_size = 64           # \n",
    "input_seq_length = 64     # Maximum length of the input sequence\n",
    "\n",
    "\n",
    "# Initialize the queries, keys, and values\n",
    "queries = np.random.random((batch_size, input_seq_length, d_k))\n",
    "keys = np.random.random((batch_size, input_seq_length, d_k))\n",
    "values = np.random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "\n",
    "# Initialize the multi-head attention layer\n",
    "multi_head_attention_layer = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "\n",
    "# Feed the data\n",
    "output_attention = multi_head_attention_layer(queries, keys, values)\n",
    "\n",
    "print(\"Output of the multi-head attention: \\n\", output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#    TRANSFORMER ENCODER    #\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the encoder: \n",
      " tf.Tensor(\n",
      "[[[ 0.37819418  0.12199633 -0.12122659 ...  1.2220829   0.9045144\n",
      "   -1.1083595 ]\n",
      "  [-0.70115393 -0.73868483  1.1814679  ...  1.0412371   0.18032189\n",
      "    0.37272906]\n",
      "  [-0.36027443 -1.2345542   0.28285587 ...  1.7093079   0.39469686\n",
      "   -0.08427565]\n",
      "  ...\n",
      "  [-0.7033425  -0.81434023  0.2103533  ...  0.7067392   0.47598606\n",
      "   -0.34587842]\n",
      "  [-0.71203476 -0.01993795 -0.04863934 ...  0.8154599   0.00322881\n",
      "   -0.15144242]\n",
      "  [-0.22229958 -0.8755953   1.2803488  ...  1.049097    0.12476938\n",
      "   -0.78157794]]\n",
      "\n",
      " [[-0.729063   -0.5781892   0.36787152 ...  1.5686175   0.7096653\n",
      "    0.02643431]\n",
      "  [-0.1265513  -0.3757296   0.3155213  ...  1.2185204   0.62619156\n",
      "   -0.25125715]\n",
      "  [-0.95256996  0.37783024  0.41807356 ...  1.8436189   1.0409188\n",
      "    0.01972632]\n",
      "  ...\n",
      "  [-0.69148254 -1.6627825   0.14145178 ...  0.13539803  0.67895925\n",
      "    0.36161864]\n",
      "  [-0.87217855 -0.09892818  0.24182941 ...  1.5428499  -0.5753602\n",
      "   -0.15251134]\n",
      "  [-1.1214936  -0.28015083  0.3957598  ...  0.94978946 -0.2035951\n",
      "   -0.16778506]]\n",
      "\n",
      " [[-0.43682262 -0.43509087 -0.1948785  ...  1.856937   -0.35677117\n",
      "   -0.44653505]\n",
      "  [-0.49660063 -0.55517143  0.14136976 ...  1.2533338  -0.29606903\n",
      "   -0.17788625]\n",
      "  [ 0.37515232 -0.92677426  1.0784605  ...  1.5564362   0.30582905\n",
      "   -0.73100907]\n",
      "  ...\n",
      "  [-0.06053089 -0.20710392  0.31170875 ...  1.3847474   0.5805817\n",
      "    0.21361218]\n",
      "  [-0.5138834  -0.7006148   0.41007447 ...  0.5603504   0.13456538\n",
      "   -0.50761247]\n",
      "  [ 0.42794383  0.04517832  0.297968   ...  0.7796374   0.5707272\n",
      "   -0.86674744]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.8323136  -0.13436663  0.2810002  ...  1.9736044   0.20186771\n",
      "   -0.34795687]\n",
      "  [-0.43645075 -1.4568564   1.0353861  ...  1.559783    0.23693566\n",
      "   -0.93588907]\n",
      "  [-0.41255504 -1.325357    0.28460023 ...  1.5793645   0.35122147\n",
      "    0.24837664]\n",
      "  ...\n",
      "  [-0.1933238  -0.99015933  0.49805617 ...  1.4963887   0.46801865\n",
      "   -0.25433993]\n",
      "  [-1.1804656  -0.61726665  0.17706324 ...  1.2144493   0.3237421\n",
      "   -0.16994497]\n",
      "  [-0.21609208 -0.0888263  -0.27984017 ...  0.7777483  -0.02251353\n",
      "   -0.85715556]]\n",
      "\n",
      " [[ 0.06913884 -0.28662467 -0.53696585 ...  1.4886831   0.34830946\n",
      "   -0.3827247 ]\n",
      "  [-0.12793781 -0.24442703  0.22406639 ...  1.2085525   0.40709633\n",
      "   -0.18765995]\n",
      "  [-0.6287488   0.71722126  0.6760298  ...  1.1057609   0.93523353\n",
      "   -0.17852162]\n",
      "  ...\n",
      "  [-1.2046243  -0.6175584   0.38586465 ...  0.80698925  0.08801925\n",
      "   -0.83914125]\n",
      "  [-0.5036789  -0.06599683 -0.13642214 ...  0.8650291   0.3664129\n",
      "   -0.46513432]\n",
      "  [-0.31881702 -0.3289588   0.28179795 ...  1.0571455   0.1648004\n",
      "    0.2494619 ]]\n",
      "\n",
      " [[-0.94194746 -0.7955552   1.4968615  ...  1.8488415   0.1524196\n",
      "   -0.17781444]\n",
      "  [-0.8263817  -0.2801351  -0.04668307 ...  1.4681473   0.78910595\n",
      "   -0.3156599 ]\n",
      "  [-0.24266724 -1.0132743   0.3173322  ...  0.33063272  0.57353437\n",
      "    0.5647477 ]\n",
      "  ...\n",
      "  [-0.8678383   0.639574   -0.02618728 ...  0.7234975  -0.04643409\n",
      "   -0.81872493]\n",
      "  [-1.2855049  -0.43550923  0.41616032 ...  0.6346978   0.08308619\n",
      "   -0.22737227]\n",
      "  [-0.8165081   0.37901208  0.2042854  ...  0.5468107   0.21293424\n",
      "   -0.6826727 ]]], shape=(64, 64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Turn off the tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import the libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from multihead_attention import MultiHeadAttentionLayer\n",
    "from positional_encoding import PositionEmbeddingLayer, PositionEmbeddingLayerWithFixedWeights\n",
    "\n",
    "# Custom layer for Add & Norm layer\n",
    "class AddNormalization(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the layer normalization layer\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x, sublayer_x):\n",
    "\n",
    "        # Add the sublayer input and output together\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization\n",
    "        out = self.layer_norm(add)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Custom layer for Feed-Forward layer\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the dense layers\n",
    "        self.fully_connected_1 = tf.keras.layers.Dense(units=d_ff)\n",
    "        self.fully_connected_2 = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # Initialize the activation function\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x):\n",
    "\n",
    "        # Feed the data\n",
    "        x = self.fully_connected_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fully_connected_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Custom layer for Transformer Encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Attention layer\n",
    "        self.multihead_attention = MultiHeadAttentionLayer(h, d_k, d_v, d_model)\n",
    "\n",
    "        # Feed-forward layer\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "\n",
    "        # Add & Norm layer\n",
    "        self.add_norm_1 = AddNormalization()\n",
    "        self.add_norm_2 = AddNormalization()\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    # Call function\n",
    "    def call(self, x, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        multihead_output   = self.multihead_attention(x, x, x, padding_mask)\n",
    "        multihead_output   = self.dropout_1(multihead_output, training=training)\n",
    "        addnorm_output     = self.add_norm_1(x, multihead_output)\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        feedforward_output = self.dropout_2(feedforward_output, training=training)\n",
    "        addnorm_output     = self.add_norm_2(addnorm_output, feedforward_output)\n",
    "\n",
    "        return addnorm_output\n",
    "\n",
    "# Custom layer for the full model\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Positional encoding layer\n",
    "        self.positional_encoding = PositionEmbeddingLayerWithFixedWeights(sequence_length, vocab_size, d_model)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # Encoder layers (for N times)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    # Call function\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "\n",
    "        # Feed the data\n",
    "        x = self.positional_encoding(input_sentence)\n",
    "        x = self.dropout(x, training=training) \n",
    "        for i_index, i_layer in enumerate(self.encoder_layer):\n",
    "            x = i_layer(x, padding_mask, training)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Initialization\n",
    "h = 8                     # Number of self-attention heads\n",
    "d_k = 64                  # Dimension of the key and query vectors\n",
    "d_v = 64                  # Dimension of the value vectors\n",
    "d_ff = 2048               # Dimension of the inner feed-forward layer\n",
    "d_model = 512             # Dimension of the mode syb-layer' output\n",
    "n = 6                     # Number of encoder layers\n",
    "batch_size = 64           # Batch size\n",
    "dropout_rate = 0.1        # Dropout rate\n",
    "enc_vocab_size = 8192     # Encoder vocabulary size\n",
    "input_seq_length = 64     # Maximum length of the input sequence\n",
    "\n",
    "# Initialize the input sequence\n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    "\n",
    "# Encoder architecture\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "# Feed the data\n",
    "output_encoder = encoder(input_seq, None, True)\n",
    "\n",
    "print(\"Output of the encoder: \\n\", output_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/predator/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-26 14:01:25.838970: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-26 14:01:25.875735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 14:01:26.451315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading shards:   0%|          | 0/9 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtiiuae/falcon-40b-instruct\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 8\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     11\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     12\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m     13\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     17\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mGirafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDaniel: Hello, Girafatron!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGirafatron:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     eos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/pipelines/__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    787\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 788\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    789\u001b[0m     model,\n\u001b[1;32m    790\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    791\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    792\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    793\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    794\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    795\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    796\u001b[0m )\n\u001b[1;32m    798\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    799\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:479\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    476\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m     _ \u001b[39m=\u001b[39m hub_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    480\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/modeling_utils.py:2585\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   2583\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[1;32m   2584\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 2585\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   2586\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2587\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2588\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2589\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2590\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2591\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2592\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2593\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2594\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   2595\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2596\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2597\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2598\u001b[0m     )\n\u001b[1;32m   2600\u001b[0m \u001b[39m# load pt weights early so that we know which dtype to init the model under\u001b[39;00m\n\u001b[1;32m   2601\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/utils/hub.py:958\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, use_auth_token, user_agent, revision, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    956\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    959\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    960\u001b[0m             shard_filename,\n\u001b[1;32m    961\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    962\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    963\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    964\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    965\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    966\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    967\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    968\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    969\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    970\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m()\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/urllib3/response.py:525\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     chunk_amt \u001b[39m=\u001b[39m max_chunk_amt\n\u001b[0;32m--> 525\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(chunk_amt)\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    527\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
