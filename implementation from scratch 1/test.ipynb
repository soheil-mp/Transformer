{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 20:03:23.849605: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-30 20:03:24.032174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-30 20:03:24.861318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Turn of the tensorlfow logging\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "# Import the libraries\n",
    "import pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformer import TransformerModel\n",
    "from dataset import PrepareDataset\n",
    "from hyperparameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the loss\n",
    "def loss_function(target, prediction):\n",
    "    \"\"\"\n",
    "    This function calculates the loss between the target and the prediction.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - target (tf.Tensor): the target tensor\n",
    "        - prediction (tf.Tensor): the prediction tensor\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - loss (tf.Tensor): the loss between the target and the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask the padding values\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Computer the sparse categorical cross entropy loss on the unmasked values\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "\n",
    "    # Calculate the mean loss over the unmasked values\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the accuracy\n",
    "def accuracy_function(target, prediction):\n",
    "    \"\"\"\n",
    "    Function for calculating the accuracy between the target and the prediction.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - target (tf.Tensor): the target tensor\n",
    "        - prediction (tf.Tensor): the prediction tensor\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - out (tf.Tensor): the accuracy between the target and the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask the padding values\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    \n",
    "    # Calculate accuracy and apply the padding mask\n",
    "    accuracy = tf.equal(target, tf.cast(tf.argmax(prediction, axis=2), tf.int32))\n",
    "    accuracy = tf.math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the accuracy from boolean to float32\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "\n",
    "    # Calculate the mean accuracy over the unmasked values\n",
    "    out = tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for scheduling the learning ear\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    This class schedules the learning rate.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - d_model (int): the model's dimensionality\n",
    "        - warmup_steps (int): the number of warmup steps\n",
    "        \n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - learning_rate (tf.Tensor): the learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, d_model, warmup_steps=4_000, **kwargs):\n",
    "\n",
    "        # Inherite the parent's constructor\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initializations\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, step_num):\n",
    "\n",
    "        # Cast step_num to float\n",
    "        step_num = tf.cast(step_num, tf.float32)\n",
    "\n",
    "        # Linearly increase the learning rate for the first warmup_steps times, then decrease it\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        # Learning rate\n",
    "        learning_rate = (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the training step (to spped up the training process)\n",
    "@tf.function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    \"\"\"\n",
    "    This function performs a training step.\n",
    "\n",
    "    PARAMETERS\n",
    "    ==========================\n",
    "        - encoder_input (tf.Tensor): the encoder input\n",
    "        - decoder_input (tf.Tensor): the decoder input\n",
    "        - decoder_output (tf.Tensor): the decoder output\n",
    "\n",
    "    RETURNS\n",
    "    ==========================\n",
    "        - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass (to make predictions)\n",
    "        prediction = model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(decoder_output, prediction)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_function(decoder_output, prediction)\n",
    "\n",
    "    # Fetch the gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Update the metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 20:03:54.344914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:54.462873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:54.463291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:54.466461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:54.466738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:54.466884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:55.560714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:55.560981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:55.561011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-30 20:03:55.561214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-30 20:03:55.561256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the optimizer with the learning rate scheduler\n",
    "optimizer = tf.keras.optimizers.Adam(LearningRateScheduler(d_model), beta_1, beta_2, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "dataset = PrepareDataset()\n",
    "train_x, train_y, val_x, val_y, train, val, encoder_sequence_length, decoder_sequence_length, encoder_vocabulary_size, decoder_vocabulary_size = dataset(\"./dataset/english-german-both.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tf.data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val_dataset = val_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = TransformerModel(encoder_vocabulary_size, decoder_vocabulary_size, encoder_sequence_length, decoder_sequence_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerModel' object has no attribute 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49msave_model(model, \u001b[39m\"\u001b[39;49m\u001b[39mweight.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, save_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m'\u001b[39;49m, save_traces\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/keras/saving/saving_api.py:145\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     saving_lib\u001b[39m.\u001b[39msave_model(model, filepath)\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Legacy case\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49msave_model(\n\u001b[1;32m    146\u001b[0m         model,\n\u001b[1;32m    147\u001b[0m         filepath,\n\u001b[1;32m    148\u001b[0m         overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    149\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m    150\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/predator/lib/python3.9/site-packages/keras/saving/legacy/saving_utils.py:351\u001b[0m, in \u001b[0;36mtry_build_compiled_arguments\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_build_compiled_arguments\u001b[39m(model):\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    350\u001b[0m         \u001b[39mnot\u001b[39;00m version_utils\u001b[39m.\u001b[39mis_v1_layer_or_model(model)\n\u001b[0;32m--> 351\u001b[0m         \u001b[39mand\u001b[39;00m model\u001b[39m.\u001b[39;49moutputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     ):\n\u001b[1;32m    353\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mcompiled_loss\u001b[39m.\u001b[39mbuilt:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerModel' object has no attribute 'outputs'"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, \"weight.ckpt\", save_format='tf', save_traces=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the metrics monitoring\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.Mean(name=\"train_accuracy\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint object and manager (for managing multiple checkpoints)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, \"./checkpoints\", max_to_keep=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for stroing the losses\n",
    "train_loss_d = {}\n",
    "val_loss_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 09:52:55.467108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [10000,12]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-06-29 09:53:09.394366: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f9718049eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-29 09:53:09.394431: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-06-29 09:53:09.402770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loop over the epochs\n",
    "for i_epoch in range(epochs):\n",
    "\n",
    "    # Reset the metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "\n",
    "    # Report \n",
    "    print(f\"Epoch {i_epoch + 1}/{epochs}\" + \"\\n===========================================\")\n",
    "\n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for i_step, (train_batch_x, train_batch_y) in enumerate(train_dataset):\n",
    "\n",
    "        # Define the encoder/decoder input/output\n",
    "        encoder_input = train_batch_x[:, 1:]\n",
    "        decoder_input = train_batch_y[:, :-1]\n",
    "        decoder_output = train_batch_y[:, 1:]\n",
    "\n",
    "        # Perform one training step\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        # Report\n",
    "        # if (i_step % 50 == 0):  print(f\"Step {i_step + 1}/{len(train_dataset)}: loss = {train_loss.result():.4f}, accuracy = {train_accuracy.result():.4f}\")\n",
    "\n",
    "    # Loop over the validation batches\n",
    "    for val_batch_x, val_batch_y in val_dataset:\n",
    "\n",
    "        # Define the encoder/decoder input/output\n",
    "        encoder_input = val_batch_x[:, 1:]\n",
    "        decoder_input = val_batch_y[:, :-1]\n",
    "        decoder_output = val_batch_y[:, 1:]\n",
    "\n",
    "        # Forward pass (to make predictions)\n",
    "        prediction = model(encoder_input, decoder_input, training=False)\n",
    "\n",
    "        # Calculate the loass\n",
    "        loss = loss_function(decoder_output, prediction)\n",
    "\n",
    "        # Update the metrics\n",
    "        val_loss(loss)\n",
    "\n",
    "    # Report\n",
    "    print(f\"Training Loss = {train_loss.result():.4f}, Training Accuracy = {train_accuracy.result():.4f}, Validation Loss = {val_loss.result():.4f}\")\n",
    "\n",
    "    # Save the checkpoint after each epoch\n",
    "    if (i_epoch+1) % 1 == 0:\n",
    "\n",
    "        # Save the checkpoint\n",
    "        save_path = checkpoint_manager.save()\n",
    "\n",
    "        # Report\n",
    "        print(f\"Checkpoint saved at {save_path}.\")\n",
    "\n",
    "        # Save the weights\n",
    "        model.save_weights(f\"./weights/weights_{i_epoch+1}.ckpt\")\n",
    "\n",
    "        # Report\n",
    "        train_loss_d[i_epoch+1] = train_loss.result()\n",
    "        val_loss_d[i_epoch+1] = val_loss.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'predator' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n predator ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Save the loss values\n",
    "with open(\"./train_loss.pkl\", \"wb\") as file:  pickle.dump(train_loss_d, file)\n",
    "with open(\"./val_loss.pkl\", \"wb\") as file:  pickle.dump(val_loss_d, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'predator' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n predator ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Report\n",
    "print(\"Total time taken: {:.2f} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'predator' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n predator ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'predator' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n predator ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'predator' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n predator ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
